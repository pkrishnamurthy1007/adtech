{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement:\n",
    "Gdoc: https://docs.google.com/document/d/1xYIV3xRNxTjUaw1fBkwB0ujJxUH6l7Gc1Mrskzny6JI/edit?usp=sharing\n",
    "\n",
    "Trying to track ROI target by adjusting cpc - assume that RPC doesnt change w/ cpc\n",
    "```\n",
    "ROI_obs = RPC_obs / CPC_obs\n",
    "ROI_target = RPC_obs / CPC_target\n",
    "ROI_obs * CPC_obs = ROI_target * CPC_target\n",
    "CPC_target = ROI_obs/ROI_target * CPC_obs = RPC_obs / ROI_target\n",
    "```\n",
    "Currently we calculate `RPC_obs` against real data - but monietization data may be spares - or u may have to go so far in the past that it no longer apllies to present\n",
    "\n",
    "Can use lead score as way to leverage more plentiful session data - w/o relying on monetization events\n",
    "\n",
    "Can split `RPC_obs` into 2 factors and a bias term: `RPQ_obs`, `QPS_obs`, and `R0` (revenue at 0 quality).  Quality is an abstract unit - the idea is that is is derived from lead score and roughly linear w/ revenue on a session level basis.  Ideally - we want to be able to calculate quality w/in sql and aggregate it at query time so we dont have to work w/ session level data.  `QPS_obs` can be aggregated w/in a modifier bucket - using plentiful session data.  `RPQ_obs` can be aggregated accross the dataset - under the assumption that `RPQ_obs` will be stable accross an entire channel/platform/product - and possibly b/w platforms/products\n",
    "```\n",
    "RPC_obs = (RPQ_obs * QPS_obs + R0)\n",
    "CPC_target = RPC_obs / ROI_target = (RPQ_obs * QPS_obs + R0) / ROI_target\n",
    "```\n",
    "\n",
    "### Domain\n",
    "Biddable Dimensions:\n",
    "1. Location (DMA)\n",
    "2. Operating System\n",
    "3. Device Type\n",
    "4. Publisher (website ad was served on)\n",
    "5. Time of Day\n",
    "\n",
    "Split will most likely be on `(DMA,OS,Device)`.  Unsure if there will be enough session level data to support splitting on dimensions beyond that.  We might have to consider some split variables in isolation.  Alternately - we could try using a k-means or percentile-based clustering method to group our data w/in a very fine grained split.\n",
    "\n",
    "### Evaluation\n",
    "Bascially we want this `rps` estimation to capture long term trends w/o short term  noise.  Ideas:\n",
    "1. plot out rolling 1,7,14,30,60,90 day estimates of rps using regular aggregations and using the lead score stabilized rps estimate\n",
    "    - want time plots and histograms\n",
    "1. compare the rps esimation to rolling rps w/ large window\n",
    "    - at that pt - why not use that rolling rps to estimate?\n",
    "1. if the idea is that we are compensating for data deficiency - why dont we use large buckets to measure bias - and small buckets to measure variance - than combine into some metric?\n",
    "\n",
    "### Initial approach\n",
    "Will just naively use `lead_score` as a standin for quality - I think that ultimately precision at that lead score would be the best quality metric - since it basically corresponds to observed conversion rate.  But I think that is best accomplished on @sperks side.  Lets see how this goes first.\n",
    "\n",
    "For a given split - I will pull in revenue averages, bucket size, and lead score averages for each bucket.  Will compute `RPQ_obs` and `R0` by fitting a 1 var regressor w/ `y=rps_avg` and `X=[lead_score_avg]` - will use `bucket size` as sample weight.\n",
    "\n",
    "NOTE: i guess there are actually 2 differnt model scores per lead - I will pivot those out and the linear regressor will have 2 weights - 1 for each model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load environ from: `SM_ENV_BASE`\n",
      "...Success!!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "# detect if we are running from a `notebooks/*_shared` folder\n",
    "# re.match(\"not.*shared\",sys.path[0])\n",
    "if sys.path[0].endswith(\"_shared\"):\n",
    "    sys.path[0] = \"/\".join(sys.path[0].split(\"/\")[:-2])\n",
    "assert sys.path[0].endswith(\"adtech\")\n",
    "\n",
    "from utils.env import load_env_from_aws\n",
    "load_env_from_aws()\n",
    "\n",
    "\n",
    "from ds_utils.db.connectors import HealthcareDW\n",
    "from notebooks.aduriseti_shared.utils import *\n",
    "\n",
    "import functools\n",
    "import datetime\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "start_date = NOW-30*DAY\n",
    "end_date = NOW\n",
    "# start_date = NOW-60*DAY\n",
    "# end_date = NOW-30*DAY\n",
    "product=None\n",
    "traffic_source = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:query: select * from data_science.materialized_session_scores limit... executed in 0.28 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>content_type</th>\n",
       "      <th>exchange</th>\n",
       "      <th>received</th>\n",
       "      <th>session_id</th>\n",
       "      <th>computed_dt</th>\n",
       "      <th>jornayaid</th>\n",
       "      <th>score</th>\n",
       "      <th>model</th>\n",
       "      <th>score_null</th>\n",
       "      <th>score_adv</th>\n",
       "      <th>score_supp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1616887693</td>\n",
       "      <td>20210327232359.b616d7aafd3e</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>82BF6C48-BDB8-D88B-1FDA-B2C1EB1661D0</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1616887304</td>\n",
       "      <td>20210327231749.d543f78c2924</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>45C48738-2942-94C6-25C2-279135E66BCD</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1616887464</td>\n",
       "      <td>20210327232230.7adf12f20f1f</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>16A6C145-382A-D17C-4619-330AE5675D12</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1616887759</td>\n",
       "      <td>20210327232800.eb951da20188</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>56C53E30-6075-6AA4-F1E2-3AEA4EA055FE</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1616887492</td>\n",
       "      <td>20210327232227.370cfd9e3368</td>\n",
       "      <td>2021-03-27</td>\n",
       "      <td>62E8B754-726A-518E-B7B7-81B4D98A8BCD</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1607364710</td>\n",
       "      <td>20201207181023.29c0e99802c7</td>\n",
       "      <td>2020-12-07</td>\n",
       "      <td></td>\n",
       "      <td>0.1196</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1196</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1607364949</td>\n",
       "      <td>20201207181244.c4c9c7e67eea</td>\n",
       "      <td>2020-12-07</td>\n",
       "      <td></td>\n",
       "      <td>0.0904</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1607364861</td>\n",
       "      <td>20201207181318.c2066edd27c7</td>\n",
       "      <td>2020-12-07</td>\n",
       "      <td></td>\n",
       "      <td>0.0452</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1607364110</td>\n",
       "      <td>20201207175655.f91419da4241</td>\n",
       "      <td>2020-12-07</td>\n",
       "      <td></td>\n",
       "      <td>0.0660</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1607364405</td>\n",
       "      <td>20201207180556.b9b382028ae0</td>\n",
       "      <td>2020-12-07</td>\n",
       "      <td></td>\n",
       "      <td>0.0473</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           topic content_type exchange    received  \\\n",
       "0   healthcare.lead.score.scored                kraken  1616887693   \n",
       "1   healthcare.lead.score.scored                kraken  1616887304   \n",
       "2   healthcare.lead.score.scored                kraken  1616887464   \n",
       "3   healthcare.lead.score.scored                kraken  1616887759   \n",
       "4   healthcare.lead.score.scored                kraken  1616887492   \n",
       "..                           ...          ...      ...         ...   \n",
       "95  healthcare.lead.score.scored                kraken  1607364710   \n",
       "96  healthcare.lead.score.scored                kraken  1607364949   \n",
       "97  healthcare.lead.score.scored                kraken  1607364861   \n",
       "98  healthcare.lead.score.scored                kraken  1607364110   \n",
       "99  healthcare.lead.score.scored                kraken  1607364405   \n",
       "\n",
       "                     session_id computed_dt  \\\n",
       "0   20210327232359.b616d7aafd3e  2021-03-27   \n",
       "1   20210327231749.d543f78c2924  2021-03-27   \n",
       "2   20210327232230.7adf12f20f1f  2021-03-27   \n",
       "3   20210327232800.eb951da20188  2021-03-27   \n",
       "4   20210327232227.370cfd9e3368  2021-03-27   \n",
       "..                          ...         ...   \n",
       "95  20201207181023.29c0e99802c7  2020-12-07   \n",
       "96  20201207181244.c4c9c7e67eea  2020-12-07   \n",
       "97  20201207181318.c2066edd27c7  2020-12-07   \n",
       "98  20201207175655.f91419da4241  2020-12-07   \n",
       "99  20201207180556.b9b382028ae0  2020-12-07   \n",
       "\n",
       "                               jornayaid   score model  score_null score_adv  \\\n",
       "0   82BF6C48-BDB8-D88B-1FDA-B2C1EB1661D0  0.0062  None      0.0062      None   \n",
       "1   45C48738-2942-94C6-25C2-279135E66BCD  0.0000  None      0.0000      None   \n",
       "2   16A6C145-382A-D17C-4619-330AE5675D12  0.0057  None      0.0057      None   \n",
       "3   56C53E30-6075-6AA4-F1E2-3AEA4EA055FE  0.0105  None      0.0105      None   \n",
       "4   62E8B754-726A-518E-B7B7-81B4D98A8BCD  0.0200  None      0.0200      None   \n",
       "..                                   ...     ...   ...         ...       ...   \n",
       "95                                        0.1196  None      0.1196      None   \n",
       "96                                        0.0904  None      0.0904      None   \n",
       "97                                        0.0452  None      0.0452      None   \n",
       "98                                        0.0660  None      0.0660      None   \n",
       "99                                        0.0473  None      0.0473      None   \n",
       "\n",
       "   score_supp  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "..        ...  \n",
       "95       None  \n",
       "96       None  \n",
       "97       None  \n",
       "98       None  \n",
       "99       None  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.data.queries.lead_score import \\\n",
    "    refresh_session_scores, \\\n",
    "    SCHEMA as DS_SCHEMA, \\\n",
    "    SCORE_TABLE\n",
    "# df = refresh_session_scores(NOW-360*DAY,NOW)\n",
    "# df\n",
    "\n",
    "with HealthcareDW() as db:\n",
    "    df = db.to_df(f\"select * from {DS_SCHEMA}.{SCORE_TABLE} limit 100\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:query: select VERSION()... executed in 0.17 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PostgreSQL 8.0.2 on i686-pc-linux-gnu, compile...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             version\n",
       "0  PostgreSQL 8.0.2 on i686-pc-linux-gnu, compile..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with HealthcareDW() as db:\n",
    "    df = db.to_df(f\"\"\"select VERSION()\"\"\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic           healthcare.lead.score.scoredhealthcare.lead.sc...\n",
       "content_type                                                     \n",
       "exchange        krakenkrakenkrakenkrakenkrakenkrakenkrakenkrak...\n",
       "received                                             160973867501\n",
       "session_id      20210327232359.b616d7aafd3e20210327231749.d543...\n",
       "jornayaid       82BF6C48-BDB8-D88B-1FDA-B2C1EB1661D045C48738-2...\n",
       "score                                                      4.7719\n",
       "model                                                           0\n",
       "score_null                                                 4.7719\n",
       "score_adv                                                       0\n",
       "score_supp                                                      0\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with HealthcareDW() as db:\n",
    "    df = db.to_df(f\"\"\"\n",
    "       SELECT  \n",
    "           session_id,\n",
    "           COUNT(*),\n",
    "           MAX(computed_dt)\n",
    "        \n",
    "        \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:query: with rps as ( SELECT session_id, sum(revenue) AS revenue FRO... executed in 33.87 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>utc_ts</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>user_ts</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>state</th>\n",
       "      <th>dma</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210427041839.ee319fd40bcb</td>\n",
       "      <td>2021-04-27 04:18:39</td>\n",
       "      <td>-7</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2021-04-26 21:18:39</td>\n",
       "      <td>1</td>\n",
       "      <td>21.25</td>\n",
       "      <td>WA</td>\n",
       "      <td>819</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20210427041307.37630a90196a</td>\n",
       "      <td>2021-04-27 04:13:07</td>\n",
       "      <td>-5</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>2021-04-26 23:13:07</td>\n",
       "      <td>1</td>\n",
       "      <td>23.00</td>\n",
       "      <td>WI</td>\n",
       "      <td>669</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20210428022910.69d570c22f2e</td>\n",
       "      <td>2021-04-28 02:29:10</td>\n",
       "      <td>-5</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>2021-04-27 21:29:10</td>\n",
       "      <td>2</td>\n",
       "      <td>21.25</td>\n",
       "      <td>MN</td>\n",
       "      <td>613</td>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20210427184724.bcea86a22070</td>\n",
       "      <td>2021-04-27 18:47:24</td>\n",
       "      <td>-4</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2021-04-27 14:47:24</td>\n",
       "      <td>2</td>\n",
       "      <td>14.75</td>\n",
       "      <td>FL</td>\n",
       "      <td>539</td>\n",
       "      <td>15.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20210427201519.d1e77bba7a9c</td>\n",
       "      <td>2021-04-27 20:15:19</td>\n",
       "      <td>-4</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2021-04-27 16:15:19</td>\n",
       "      <td>2</td>\n",
       "      <td>16.25</td>\n",
       "      <td>MD</td>\n",
       "      <td>512</td>\n",
       "      <td>10.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>20210427220919.99e594c17260</td>\n",
       "      <td>2021-04-27 22:09:19</td>\n",
       "      <td>-7</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2021-04-27 15:09:19</td>\n",
       "      <td>2</td>\n",
       "      <td>15.00</td>\n",
       "      <td>CA</td>\n",
       "      <td>802</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>20210428031224.37284ad3405d</td>\n",
       "      <td>2021-04-28 03:12:24</td>\n",
       "      <td>-7</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2021-04-27 20:12:24</td>\n",
       "      <td>2</td>\n",
       "      <td>20.00</td>\n",
       "      <td>CA</td>\n",
       "      <td>825</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20210427141155.d758f1b01f77</td>\n",
       "      <td>2021-04-27 14:11:55</td>\n",
       "      <td>-5</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>2021-04-27 09:11:55</td>\n",
       "      <td>2</td>\n",
       "      <td>9.00</td>\n",
       "      <td>MN</td>\n",
       "      <td>613</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>20210427222246.b06003c1cdc8</td>\n",
       "      <td>2021-04-27 22:22:46</td>\n",
       "      <td>-6</td>\n",
       "      <td>America/Boise</td>\n",
       "      <td>2021-04-27 16:22:46</td>\n",
       "      <td>2</td>\n",
       "      <td>16.25</td>\n",
       "      <td>ID</td>\n",
       "      <td>757</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>20210427152236.030a77a6412d</td>\n",
       "      <td>2021-04-27 15:22:36</td>\n",
       "      <td>-4</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>2021-04-27 11:22:36</td>\n",
       "      <td>2</td>\n",
       "      <td>11.25</td>\n",
       "      <td>OH</td>\n",
       "      <td>510</td>\n",
       "      <td>16.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     session_id              utc_ts  utc_offset  \\\n",
       "0   20210427041839.ee319fd40bcb 2021-04-27 04:18:39          -7   \n",
       "1   20210427041307.37630a90196a 2021-04-27 04:13:07          -5   \n",
       "2   20210428022910.69d570c22f2e 2021-04-28 02:29:10          -5   \n",
       "3   20210427184724.bcea86a22070 2021-04-27 18:47:24          -4   \n",
       "4   20210427201519.d1e77bba7a9c 2021-04-27 20:15:19          -4   \n",
       "..                          ...                 ...         ...   \n",
       "95  20210427220919.99e594c17260 2021-04-27 22:09:19          -7   \n",
       "96  20210428031224.37284ad3405d 2021-04-28 03:12:24          -7   \n",
       "97  20210427141155.d758f1b01f77 2021-04-27 14:11:55          -5   \n",
       "98  20210427222246.b06003c1cdc8 2021-04-27 22:22:46          -6   \n",
       "99  20210427152236.030a77a6412d 2021-04-27 15:22:36          -4   \n",
       "\n",
       "              time_zone             user_ts  dayofweek  hourofday state  dma  \\\n",
       "0   America/Los_Angeles 2021-04-26 21:18:39          1      21.25    WA  819   \n",
       "1       America/Chicago 2021-04-26 23:13:07          1      23.00    WI  669   \n",
       "2       America/Chicago 2021-04-27 21:29:10          2      21.25    MN  613   \n",
       "3      America/New_York 2021-04-27 14:47:24          2      14.75    FL  539   \n",
       "4      America/New_York 2021-04-27 16:15:19          2      16.25    MD  512   \n",
       "..                  ...                 ...        ...        ...   ...  ...   \n",
       "95  America/Los_Angeles 2021-04-27 15:09:19          2      15.00    CA  802   \n",
       "96  America/Los_Angeles 2021-04-27 20:12:24          2      20.00    CA  825   \n",
       "97      America/Chicago 2021-04-27 09:11:55          2       9.00    MN  613   \n",
       "98        America/Boise 2021-04-27 16:22:46          2      16.25    ID  757   \n",
       "99     America/New_York 2021-04-27 11:22:36          2      11.25    OH  510   \n",
       "\n",
       "    revenue  \n",
       "0     15.88  \n",
       "1      1.91  \n",
       "2      3.09  \n",
       "3     15.94  \n",
       "4     10.72  \n",
       "..      ...  \n",
       "95     3.84  \n",
       "96     2.61  \n",
       "97     3.14  \n",
       "98     1.53  \n",
       "99    16.27  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "product_filter = \"\" if product is None else \\\n",
    "    f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "traffic_filter = \"\" if traffic_source is None else \\\n",
    "    f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "session_revenue_sql = f\"\"\"\n",
    "SELECT\n",
    "    session_id,\n",
    "    sum(revenue) AS revenue\n",
    "FROM tron.session_revenue\n",
    "WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "    {product_filter}\n",
    "    {traffic_filter}\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "geoip_sql = f\"\"\"\n",
    "SELECT \n",
    "    l.*,\n",
    "    b.netowrk_index,\n",
    "    b.start_int,\n",
    "    b.end_int\n",
    "FROM \n",
    "    data_science.maxmind_ipv4_geo_blocks AS b\n",
    "    JOIN data_science.maxmind_geo_locations AS l\n",
    "        ON b.maxmind_id = l.maxmind_id\n",
    "\"\"\"\n",
    "lead_score_sql = f\"\"\"\n",
    "SELECT\n",
    "    r.topic,\n",
    "    r.content_type,\n",
    "    r.exchange,\n",
    "    r.received,\n",
    "    r.body.sessionid                        AS session_id,\n",
    "    TO_DATE(r.body.\"on\", 'YYYY-MM-DD')      AS computed_dt,\n",
    "    r.body.jornayaid,\n",
    "    r.body.score                            AS score,\n",
    "    r.body.response.meta.model              AS model,\n",
    "    CASE \n",
    "        WHEN model IS NULL THEN score \n",
    "        ELSE null\n",
    "    END                                     AS score_null,\n",
    "    CASE \n",
    "        WHEN model = 'med-adv' THEN score \n",
    "        ELSE null   \n",
    "    END                                     AS score_adv,\n",
    "    CASE \n",
    "        WHEN model = 'med-supp' THEN score \n",
    "        ELSE null   \n",
    "    END                                     AS score_supp\n",
    "FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "WHERE\n",
    "    /* Data partitioned on date - these filters greatly speed query */\n",
    "    (r.year > {start_date.year} OR \n",
    "        (r.year = {start_date.year} AND r.month >= {start_date.month})) \n",
    "    AND\n",
    "    (r.year < {end_date.year} OR \n",
    "        (r.year = {end_date.year} AND r.month <= {end_date.month}))\n",
    "\"\"\"\n",
    "sql_query = f\"\"\"\n",
    "WITH\n",
    "    rps AS ({session_revenue_sql}),\n",
    "    ip_locs AS ({geoip_sql}),\n",
    "    lead_scores AS ({session_revenue_sql}),\n",
    "    rps_tz_adj AS (\n",
    "        SELECT\n",
    "            s.session_id,\n",
    "            s.creation_date                                         AS utc_ts,\n",
    "            extract(\n",
    "                HOUR FROM\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                    - s.creation_date\n",
    "            )::INT                                                  AS utc_offset,\n",
    "            l.time_zone,\n",
    "            convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "            date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "            date_part(HOUR, user_ts) +\n",
    "            CASE \n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "            END                                                     AS hourofday,\n",
    "            l.subdivision_1_iso_code                                AS state,\n",
    "            l.metro_code                                            AS dma,\n",
    "            r.revenue\n",
    "        FROM \n",
    "            tracking.session_detail AS s\n",
    "            JOIN ip_locs as l\n",
    "                ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                AND l.country_iso_code = 'US'\n",
    "            INNER JOIN rps as r\n",
    "                ON s.session_id = r.session_id\n",
    "            LEFT JOIN lead_scores as scores\n",
    "                ON s.session_id = scores.session_id\n",
    "        WHERE nullif(s.ip_address, '') IS NOT null\n",
    "            AND nullif(dma,'') IS NOT NULL \n",
    "            AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "    )\n",
    "SELECT\n",
    "    *\n",
    "FROM \n",
    "    rps_tz_adj\n",
    "LIMIT 100\n",
    ";\n",
    "\"\"\"\n",
    "with HealthcareDW() as db:\n",
    "    session_rps_df = db.to_df(sql_query)\n",
    "session_rps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_filter = \"\" if product is None else \\\n",
    "    f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "traffic_filter = \"\" if traffic_source is None else \\\n",
    "    f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "session_revenue_sql = f\"\"\"\n",
    "SELECT\n",
    "    session_id,\n",
    "    sum(revenue) AS revenue\n",
    "FROM tron.session_revenue\n",
    "WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "    {product_filter}\n",
    "    {traffic_filter}\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "geoip_sql = f\"\"\"\n",
    "SELECT \n",
    "    l.*,\n",
    "    b.netowrk_index,\n",
    "    b.start_int,\n",
    "    b.end_int\n",
    "FROM \n",
    "    data_science.maxmind_ipv4_geo_blocks AS b\n",
    "    JOIN data_science.maxmind_geo_locations AS l\n",
    "        ON b.maxmind_id = l.maxmind_id\n",
    "\"\"\"\n",
    "unified\n",
    "agg_rps_query = f\"\"\"\n",
    "with\n",
    "    rps as ({session_revenue_sql}),\n",
    "    ip_locs as ({geoip_sql}),\n",
    "    rps_tz_adj as (\n",
    "        SELECT\n",
    "            s.creation_date                                         AS utc_ts,\n",
    "            extract(\n",
    "                HOUR FROM\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                    - s.creation_date\n",
    "            )::INT                                                  AS utc_offset,\n",
    "            l.time_zone,\n",
    "            convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "            date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "            date_part(HOUR, user_ts) +\n",
    "            CASE \n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "            END                                                     AS hourofday,\n",
    "            l.subdivision_1_iso_code                                AS state,\n",
    "            l.metro_code                                            AS dma,\n",
    "            r.revenue\n",
    "        FROM \n",
    "            tracking.session_detail AS s\n",
    "            JOIN ip_locs as l\n",
    "                ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                AND l.country_iso_code = 'US'\n",
    "            INNER JOIN rps as r\n",
    "                ON s.session_id = r.session_id\n",
    "        WHERE nullif(s.ip_address, '') IS NOT null\n",
    "            AND nullif(dma,'') IS NOT NULL \n",
    "            AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "    )\n",
    "SELECT\n",
    "    {','.join(agg_columns)},\n",
    "    COUNT(session_id)                                                       AS sessions,\n",
    "    SUM((revenue>0)::INT::FLOAT)                                            AS num_leads,\n",
    "    AVG((revenue>0)::INT::FLOAT)                                            AS lps_avg,\n",
    "    SUM(revenue) / CASE\n",
    "        WHEN num_leads = 0 THEN 1\n",
    "        ELSE num_leads\n",
    "    END                                                                     AS rpl_avg,\n",
    "    (SUM(revenue) / COUNT(DISTINCT session_id))::NUMERIC(8,4)               AS rps_,\n",
    "    AVG(revenue)                                                            AS rps_avg,\n",
    "    STDDEV(revenue)                                                         AS rps_std,\n",
    "    VARIANCE(revenue)                                                       AS rps_var\n",
    "FROM rps_tz_adj\n",
    "GROUP BY {','.join(agg_columns)}\n",
    "\"\"\"\n",
    "# print(agg_rps_query)\n",
    "# print(traffic_filter)\n",
    "from ds_utils.db.connectors import HealthcareDW\n",
    "with HealthcareDW() as db:\n",
    "    df = db.to_df(agg_rps_query)\n",
    "globals()[\"df\"] = df\n",
    "\n",
    "delt = df[\"rps_avg\"] - df['rps_']\n",
    "if not all(delt.abs() < 1e-3):\n",
    "    print(\"session uniqueness assummption not satisfied\")\n",
    "df = df \\\n",
    "    .sort_values(by=agg_columns, ascending=True) \\\n",
    "    .set_index(agg_columns)\n",
    "\n",
    "df['int_ix'] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "import functools\n",
    "@functools.lru_cache()\n",
    "def agg_rps(start_date,end_date,product,traffic_source,agg_columns):\n",
    "    agg_columns = list(agg_columns)\n",
    "    product_filter = \"\" if product is None else \\\n",
    "        f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "    traffic_filter = \"\" if traffic_source is None else \\\n",
    "        f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "    agg_rps_query = f\"\"\"\n",
    "    with\n",
    "        rps as (\n",
    "            SELECT\n",
    "                session_id,\n",
    "                sum(revenue) AS revenue\n",
    "            FROM tron.session_revenue\n",
    "            WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "                {product_filter}\n",
    "                {traffic_filter}\n",
    "            GROUP BY 1\n",
    "        ),\n",
    "        ip_locs as (\n",
    "            SELECT \n",
    "                l.*,\n",
    "                b.netowrk_index,\n",
    "                b.start_int,\n",
    "                b.end_int\n",
    "            FROM \n",
    "                data_science.maxmind_ipv4_geo_blocks AS b\n",
    "                JOIN data_science.maxmind_geo_locations AS l\n",
    "                    ON b.maxmind_id = l.maxmind_id\n",
    "        ),\n",
    "        rps_tz_adj as (\n",
    "            SELECT\n",
    "                s.*,\n",
    "                s.creation_date                                         AS utc_ts,\n",
    "                extract(\n",
    "                    HOUR FROM\n",
    "                    convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                        - s.creation_date\n",
    "                )::INT                                                  AS utc_offset,\n",
    "                l.time_zone,\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "                date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "                date_part(HOUR, user_ts) +\n",
    "                CASE \n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "                END                                                     AS hourofday,\n",
    "                l.subdivision_1_iso_code                                AS state,\n",
    "                l.metro_code                                            AS dma,\n",
    "                r.revenue\n",
    "            FROM \n",
    "                tracking.session_detail AS s\n",
    "                JOIN ip_locs as l\n",
    "                    ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                    AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                    AND l.country_iso_code = 'US'\n",
    "                INNER JOIN rps as r\n",
    "                    ON s.session_id = r.session_id\n",
    "            WHERE nullif(s.ip_address, '') IS NOT null\n",
    "                AND nullif(dma,'') IS NOT NULL \n",
    "                AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "                {product_filter}\n",
    "                {traffic_filter}\n",
    "        )\n",
    "    SELECT\n",
    "        {','.join(agg_columns)},\n",
    "        COUNT(session_id)                                                       AS sessions,\n",
    "        SUM((revenue>0)::INT::FLOAT)                                            AS num_leads,\n",
    "        AVG((revenue>0)::INT::FLOAT)                                            AS lps_avg,\n",
    "        SUM(revenue) / CASE\n",
    "            WHEN num_leads = 0 THEN 1\n",
    "            ELSE num_leads\n",
    "        END                                                                     AS rpl_avg,\n",
    "        (SUM(revenue) / COUNT(DISTINCT session_id))::NUMERIC(8,4)               AS rps_,\n",
    "        AVG(revenue)                                                            AS rps_avg,\n",
    "        STDDEV(revenue)                                                         AS rps_std,\n",
    "        VARIANCE(revenue)                                                       AS rps_var\n",
    "    FROM rps_tz_adj\n",
    "    GROUP BY {','.join(agg_columns)}\n",
    "    \"\"\"\n",
    "    # print(agg_rps_query)\n",
    "    # print(traffic_filter)\n",
    "    from ds_utils.db.connectors import HealthcareDW\n",
    "    with HealthcareDW() as db:\n",
    "        df = db.to_df(agg_rps_query)\n",
    "    globals()[\"df\"] = df\n",
    "\n",
    "    delt = df[\"rps_avg\"] - df['rps_']\n",
    "    if not all(delt.abs() < 1e-3):\n",
    "        print(\"session uniqueness assummption not satisfied\")\n",
    "    df = df \\\n",
    "        .sort_values(by=agg_columns, ascending=True) \\\n",
    "        .set_index(agg_columns)\n",
    "\n",
    "    df['int_ix'] = range(len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "campaign_split_fields = dict(\n",
    "    # traffic_source = [\"traffic_source\"],\n",
    "    browser = [\"browser\"],\n",
    "    operating_system = [\"operating_system\"],\n",
    "    device = [\"device\"],\n",
    "    # channel = [\"channel\"],\n",
    "    # domain = [\"domain\"],\n",
    "    product = [\"product\"],\n",
    "    # keyword = [\"keyword\"],\n",
    "    # campaign_id = [\"campaign_id\"],\n",
    "    # landing_page = [\"landing_page\"],\n",
    "    TOD = [\"dayofweek\",\"hourofday\"],\n",
    "    dma = [\"dma\"],\n",
    "    state =[\"state\",],\n",
    "    location = [\"state\",\"dma\"],\n",
    "    \n",
    "    dma_os=[\"dma\", \"operating_system\"],\n",
    "    dma_device=[\"dma\", \"device\", ],\n",
    "    dma_os_device=[\"dma\", \"operating_system\", \"device\"],\n",
    "\n",
    "    state_os=[\"state\", \"operating_system\"],\n",
    "    state_device=[\"state\", \"device\", ],\n",
    "    state_os_device=[\"state\", \"operating_system\", \"device\"],\n",
    "\n",
    "    location_os = [\"state\", \"dma\", \"operating_system\"],\n",
    "    location_device=[\"state\", \"dma\", \"device\", ],\n",
    "    location_os_device = [\"state\", \"dma\", \"operating_system\",\"device\"],\n",
    ")\n",
    "\n",
    "taboola_val_map = {\n",
    "    \"device\": {\n",
    "        'DESKTOP': 'DESK',\n",
    "        'MOBILE': 'PHON',\n",
    "        'TABLET': 'TBLT',\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        'Linux armv7l': \"Linux\",\n",
    "        'Linux armv8l': \"Linux\",\n",
    "        'Linux x86_64': \"Linux\",\n",
    "        'MacIntel': 'Mac OS X',\n",
    "        'Win32': \"Windows\",\n",
    "        'iPad': \"iPadOS\",\n",
    "        'iPhone': \"iOS\",\n",
    "        '': None,\n",
    "        'ARM': None,\n",
    "        'Android': 'Android',\n",
    "        'Linux aarch64': \"Linux\",\n",
    "        'Win64': \"Windows\",\n",
    "        'Linux armv7': \"Linux\",\n",
    "        'Linux i686': \"Linux\",\n",
    "        'Windows': \"Windows\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def translate_taboola_vals(df):\n",
    "    index_cols = df.index.names\n",
    "    df = df.reset_index()\n",
    "    for c in df.columns:\n",
    "        if c in taboola_val_map:\n",
    "            df[c] = df[c].apply(taboola_val_map[c].__getitem__)\n",
    "    df_bkp = df\n",
    "    df = df \\\n",
    "        .groupby(index_cols) \\\n",
    "        .agg({\n",
    "            \"sessions\": sum,\n",
    "            \"num_leads\": sum,\n",
    "            \"lps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rpl_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "        })\n",
    "    df[\"int_ix\"] = range(len(df))\n",
    "    df_bkp_wavg = wavg(df_bkp[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                        df_bkp[\"sessions\"].values.reshape(-1, 1))\n",
    "    df_wavg = wavg(df[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                    df[\"sessions\"].values.reshape(-1, 1))\n",
    "    assert all((df_bkp_wavg - df_wavg).abs() < 1e-2), (df_bkp_wavg,df_wavg)\n",
    "    return df\n",
    "\n",
    "import pprint\n",
    "from IPython.display import display as ipydisp    \n",
    "import pandas as pd\n",
    "from models.utils import wavg\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def wvar(V,W):\n",
    "    mu = wavg(V,W)\n",
    "    var = wavg((V - mu)**2,W)\n",
    "    return var\n",
    "def wstd(V,W):\n",
    "    return wvar(V,W)**0.5\n",
    "def get_wavg_by(df, col):\n",
    "    def wavg_by(V):\n",
    "        return wavg(V, W=df.loc[V.index, col])\n",
    "    return wavg_by\n",
    "\n",
    "def get_wthresh(W,p):\n",
    "    W = rps_df[\"sessions\"].sort_values(ascending=False)\n",
    "    Wsum = W.sum()\n",
    "    cumsum = 0\n",
    "    for wthresh in W:\n",
    "        if cumsum > Wsum * p:\n",
    "            break\n",
    "        cumsum += wthresh\n",
    "    return wthresh\n",
    "\n",
    "\"\"\"\n",
    "TODO: 2021-05-20\n",
    "  - test fitting on general traffic sources?\n",
    "  - test clustering on multiple days of rps data - \n",
    "      or rolling rps data or something\n",
    "  - try out more granular aggs\n",
    "  - is there some kind of metric which measures total variance\n",
    "      and computes how much of that variance is captured by a split?\n",
    "\n",
    "Trevor: 2021-05-21\n",
    "TODO:\n",
    "- figure out how to minimize campaign # when writing back to taboola\n",
    "    - want to make sure campaigns have sufficient traffic\n",
    "- kw=(location,os,device)\n",
    "- calc 30-day rps/kw\n",
    "- can create distribution over the rps(kw) distribution\n",
    "- 100 campaigns - 1 per percentile\n",
    "\n",
    "TODO: 2021-05-24\n",
    "- look into clustering each individual split variable\n",
    "\"\"\"\n",
    "\n",
    "# split2aggrps = {}\n",
    "# for split,split_cols in campaign_split_fields.items():\n",
    "#     print(split,split_cols)\n",
    "#     rps_df = agg_rps(NOW-90*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=split_cols)\n",
    "#     rps_df = translate_taboola_vals(rps_df)\n",
    "#     rps_df[\"split_on\"] = split\n",
    "#     split2aggrps[split] = rps_df\n",
    "#     print(split,rps_df.shape)\n",
    "\n",
    "\n",
    "def agg_rps(start_date, end_date, product, traffic_source, agg_columns):\n",
    "\n",
    "\n",
    "def agg_rps_taboola(start_date, end_date, product, traffic_source, agg_columns):\n",
    "    rps_df = agg_rps(start_date,end_date,None,traffic_source=traffic_source,agg_columns=agg_columns)\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "\n",
    "split2fitaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-90*DAY,NOW-30*DAY,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2fitaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n",
    "\n",
    "split2evalaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-30*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2evalaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.cluster\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "def cluster_split(split):\n",
    "    rps_df_fit = split2fitaggrps[split]\n",
    "    rps_df_eval = split2evalaggrps[split]\n",
    "    split_size = rps_df_fit.__len__()\n",
    "    MINCLUST = 8\n",
    "    if split_size <= MINCLUST:\n",
    "        rps_clust_df = rps_df_eval\n",
    "        rps_df_fit[\"clust\"] = 0\n",
    "        rps_df_eval[\"clust\"] = 0\n",
    "    else:\n",
    "        # nclust = MINCLUST\n",
    "        nclust = max(MINCLUST, int(np.log(split_size)))\n",
    "        # nclust = max(MINCLUST,int(split_size ** 0.5))\n",
    "        # print(\"nclust\", nclust, split_size, np.log(split_size))\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=nclust)\n",
    "        rps_df_fit[\"clust\"] = clusterer \\\n",
    "            .fit_predict(\n",
    "                rps_df_fit[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_fit[\"sessions\"])\n",
    "        rps_df_eval[\"clust\"] = clusterer \\\n",
    "            .predict(\n",
    "                rps_df_eval[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_eval[\"sessions\"])\n",
    "        rps_clust_df = rps_df_eval \\\n",
    "            .groupby(\"clust\") \\\n",
    "            .agg({\"rps_avg\": get_wavg_by(rps_df_eval, \"sessions\"), \"sessions\": sum})\n",
    "    assert rps_clust_df[\"rps_avg\"].max() <= rps_df_eval[\"rps_avg\"].max()\n",
    "    rps_wavg = wavg(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"])\n",
    "    rps_clust_wavg = wavg(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"])\n",
    "    assert abs(rps_wavg - rps_clust_wavg) < 1e-4, (rps_wavg, rps_clust_wavg)\n",
    "    return rps_df_fit,rps_df_eval,rps_clust_df\n",
    "\n",
    "def get_split_factor(rps_df):\n",
    "    orig_index = rps_df.index.names\n",
    "    split_attr2unique_vals = {index_col: rps_df.index.unique(\n",
    "        index_col) for index_col in orig_index}\n",
    "    _,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "    nclusts = rps_df[\"clust\"].unique().__len__()\n",
    "    split_factor = nclusts * np.prod([1] + [split_attr2unique_vals[c].__len__() for c in new_index_order[:-1]])\n",
    "    return split_factor\n",
    "\n",
    "perfD = []\n",
    "for split in campaign_split_fields.keys():\n",
    "    rps_df_fit,rps_df_eval,rps_clust_df = cluster_split(split)\n",
    "    perfd = {\n",
    "        \"split\": split,\n",
    "        \"fit_shape\": rps_df_fit.shape,\n",
    "        \"clust_shape\": rps_clust_df.shape,\n",
    "        # wavg(rps_df[\"rps_avg\"],rps_df[\"sessions\"]),\n",
    "        \"split_variance\": wstd(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"]),\n",
    "        \"cluster_variance\": wstd(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"]),\n",
    "        # wstd(rps_df[\"rps_avg\"],rps_df[\"sessions\"])\n",
    "        \"split_factor\": rps_df_fit.__len__(),\n",
    "        \"clustered_split_factor\": get_split_factor(rps_df_fit),\n",
    "    }\n",
    "    perfD.append(perfd)\n",
    "    pprint.pprint(perfd)\n",
    "    ipydisp(rps_clust_df)\n",
    "\n",
    "perfdf = pd.DataFrame(perfD)\n",
    "ipydisp(perfdf)\n",
    "#%%\n",
    "# BEST_SPLIT = \"location_os_device\"\n",
    "BEST_SPLIT = \"dma_os_device\"\n",
    "# BEST_SPLIT = \"dma_os\"\n",
    "\n",
    "rps_df_fit, rps_df_eval, rps_clust_df = cluster_split(BEST_SPLIT)\n",
    "rps_df = rps_df_fit\n",
    "get_split_factor(rps_df)\n",
    "orig_index = rps_df.index.names\n",
    "split_attr2unique_vals = {index_col: rps_df.index.unique(index_col) for index_col in orig_index}\n",
    "_,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "rps_df = rps_df .reset_index()\n",
    "campaign_df = rps_df \\\n",
    "    .groupby([*new_index_order[:-1], \"clust\"]) \\\n",
    "    .agg({\n",
    "        \"sessions\": sum,\n",
    "        \"rps_avg\": get_wavg_by(rps_df,\"sessions\"),\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "assert campaign_df[\"sessions\"].sum() == rps_df_fit[\"sessions\"].sum()\n",
    "camp_rps_wavg = wavg(campaign_df[\"rps_avg\"],campaign_df[\"sessions\"])\n",
    "fit_rps_wavg = wavg(rps_df_fit[\"rps_avg\"], rps_df_fit[\"sessions\"])\n",
    "assert abs(camp_rps_wavg - fit_rps_wavg) < 1e-5\n",
    "\n",
    "excl_campaign_df = campaign_df.groupby([*new_index_order[:-1]]) \\\n",
    "    .agg({\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "def flatten(M):\n",
    "    return tuple(el for r in M for el in r)\n",
    "excl_campaign_df[new_index_order[-1]] = excl_campaign_df[new_index_order[-1]] .apply(flatten)\n",
    "\n",
    "camps = []\n",
    "for idx,r in campaign_df.iterrows():\n",
    "    camp = {\n",
    "        \"sessions_60d\": r[\"sessions\"], \n",
    "        \"rps_avg_60d\": r[\"rps_avg\"]\n",
    "    }\n",
    "    for field,val in zip(new_index_order[:-1],idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"includes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "for idx, r in excl_campaign_df.iterrows():\n",
    "    camp = {}\n",
    "    for field, val in zip(new_index_order[:-1], idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"excludes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "camp_df = pd.DataFrame(camps)\n",
    "camp_df.to_csv(\"campaign_dump.csv\")\n",
    "#%%\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn.feature_selection\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "\n",
    "requires_campaign_split = [\n",
    "    # \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    # \"domain\",\n",
    "    \"product\",\n",
    "    # \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "\n",
    "for split in requires_campaign_split:\n",
    "    rps_df = split2aggrps[split] .reset_index()\n",
    "    Xs = rps_df \\\n",
    "        .apply(lambda r: [r[\"int_ix\"]]*int(r[\"sessions\"]), axis=1)\n",
    "    X = np.concatenate(Xs.values).reshape(-1,1)\n",
    "    ys = rps_df \\\n",
    "        .apply(lambda r: [r[\"rps_avg\"]]*int(r[\"sessions\"]),axis=1)\n",
    "    y = np.concatenate(ys.values)\n",
    "    # print(y.min(),np.quantile(y, 0.5),y.max())\n",
    "    y = y > y.mean()\n",
    "    # y = np.concatenate(ys.values).reshape(-1,1)\n",
    "    # y = sklearn.preprocessing.KBinsDiscretizer(n_bins=2,encode=\"ordinal\") \\\n",
    "    #     .fit_transform(y).reshape(-1)\n",
    "    mi = sklearn.feature_selection.mutual_info_regression(X,y,discrete_features=True)\n",
    "    print(split,mi,y.mean(),rps_df.shape)\n",
    "#%%\n",
    "y\n",
    "#%%\n",
    "sklearn.metrics.mutual_info_score\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "requires_campaign_split = [\n",
    "    \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    # \"domain\",\n",
    "    \"product\",\n",
    "    # \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "combined_rps_df = pd.concat([df.reset_index() for df in split2aggrps.values()])\n",
    "splitI = combined_rps_df[\"split_on\"].isin(requires_campaign_split)\n",
    "density = scipy.stats.gaussian_kde(\n",
    "    dataset=combined_rps_df.loc[splitI,\"rps_avg\"],\n",
    "    weights=combined_rps_df.loc[splitI, \"sessions\"],\n",
    "    # bw_method=\"scott\",\n",
    "    # bw_method=\"silverman\",\n",
    "    # bw_method=0.1,\n",
    ")\n",
    "xs = np.linspace(0,2,100)\n",
    "plt.plot(xs,density(xs))\n",
    "#%%\n",
    "for split in requires_campaign_split:\n",
    "    splitI = combined_rps_df[\"split_on\"] == split\n",
    "    density = scipy.stats.gaussian_kde(\n",
    "        dataset=combined_rps_df.loc[splitI, \"rps_avg\"],\n",
    "        weights=combined_rps_df.loc[splitI, \"sessions\"])\n",
    "    xs = np.linspace(0, 2, 100)\n",
    "    plt.plot(xs, density(xs))\n",
    "    plt.title(split)\n",
    "    plt.show()\n",
    "#%%\n",
    "combined_rps_df.loc[splitI, [\"rps_avg\",\"sessions\"]].apply(\n",
    "        lambda r: pd.Series([r[\"rps_avg\"]]*int(r[\"sessions\"])),axis=1) \\\n",
    "    .unstack()\n",
    "#%%\n",
    "agg_rps = split2aggrps[\"TOD\"]\n",
    "Xy = agg_rps[[\"sessions\",\"rps\"]].reset_index()\n",
    "Xy[\"i\"] = range(len(Xy))\n",
    "Xy\n",
    "#%%\n",
    "import sklearn.feature_selection\n",
    "sklearn.feature_selection.mutual_info_regression(\n",
    "    []\n",
    ")\n",
    "\n",
    "#%%\n",
    "\"\"\"\n",
    "- \n",
    "- overall goal:\n",
    "    - specific ROI targetting w/ minimal campaigns\n",
    "- what this means for accnt structure\n",
    "    - for many variables we must split campaigns to target ROI\n",
    "    - want to capture greatest amt of rps variation w/ \n",
    "        fewest # of campaigns\n",
    "- 2 ways of approaching this\n",
    "    1. minimize rps variation w/in a campaign\n",
    "        - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "        - tried this out a little - but the computation was intensive which made it slow going\n",
    "    2. maximize rps variation outside campaigns\n",
    "    - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "    - mostly focused on this approach\n",
    "\n",
    "- approaches I tried\n",
    "1. rank splits by their correlation/covaraince/MI w/ `agg_rps`\n",
    "    - tried ANOVA,pt.biserial,MI - had most success w/ MI\n",
    "    - was dificult to compute b/c couldnt find methods that accepted sample weight\n",
    "    - MI approach was promising but didnt go down that route\n",
    "2. rank splits by intra-split agg_rps variance\n",
    "    - had most success w/ this\n",
    "    - tested out an approahc where I cluster the split on rps - had good results\n",
    "    - think this is the mtd to use going forward\n",
    "    TODO:\n",
    "    - test fitting on general traffic sources?\n",
    "    - test clustering on multiple days of rps data - \n",
    "        or rolling rps data or something\n",
    "    - try out more granular aggs\n",
    "    - is there some kind of metric which measures total variance\n",
    "        and computes how much of that variance is captured by a split?\n",
    "\n",
    "3. fit decision tree on rps data \n",
    "    - MSE criterion is apparently the same as minimizng inter split variance\n",
    "    - computationally intensive\n",
    "    - not 100% clear how to go from tree to campaign structure\n",
    "\n",
    "\n",
    "- can do this by\n",
    "    1. choosing what vars or tuples of vars to split campaigns on\n",
    "    2. grouping similar buckets w/in those splits\n",
    "- want to measure correlation/dependence of categorical split vars w/ cont rps\n",
    "    - cat,cat metrics w/ binned rps\n",
    "        chi\n",
    "    - cont,cont metrcs w/ 1 hot encoded split vars\n",
    "    - cat,cont metrics\n",
    "- correlation metrics\n",
    "    - MI:\n",
    "        - sklearn.feature_selection.mi_regression\n",
    "        - would need AFAICT to rresample input arrays\n",
    "    - ANOVA:\n",
    "        - scipy.stats.f_oneway\n",
    "    - pt biserial\n",
    "        - needs binary vars tho\n",
    "        - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html\n",
    "        - https://towardsdatascience.com/point-biserial-correlation-with-python-f7cd591bd3b1\n",
    "- want most variation of rps w/ fewest camapaigns\n",
    "- dont want to group similar buckets together - think its too complicated\n",
    "\n",
    "\"\"\"\n",
    "#%%\n",
    "requires_campaign_split = [\n",
    "    \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    \"domain\",\n",
    "    \"product\",\n",
    "    \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "#%%\n",
    "\n",
    "traffic_source = TABOOLA\n",
    "with HealthcareDW() as db:\n",
    "    traffic_filter = \"\" if traffic_source is None else \\\n",
    "        f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "    sql = f\"\"\"\n",
    "    select\n",
    "        traffic_source,domain,count(*)\n",
    "    from tracking.session_detail\n",
    "    where True \n",
    "    {traffic_filter}\n",
    "    group by traffic_source,domain;\n",
    "    \"\"\"\n",
    "    df = db.to_df(sql).sort_values(\"count\")\n",
    "df\n",
    "#%%\n",
    "\n",
    "# %%\n",
    "with HealthcareDW() as db:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
