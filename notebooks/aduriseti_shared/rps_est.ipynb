{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement:\n",
    "Gdoc: https://docs.google.com/document/d/1xYIV3xRNxTjUaw1fBkwB0ujJxUH6l7Gc1Mrskzny6JI/edit?usp=sharing\n",
    "\n",
    "Trying to track ROI target by adjusting cpc - assume that RPC doesnt change w/ cpc\n",
    "```\n",
    "ROI_obs = RPC_obs / CPC_obs\n",
    "ROI_target = RPC_obs / CPC_target\n",
    "ROI_obs * CPC_obs = ROI_target * CPC_target\n",
    "CPC_target = ROI_obs/ROI_target * CPC_obs = RPC_obs / ROI_target\n",
    "```\n",
    "Currently we calculate `RPC_obs` against real data - but monietization data may be spares - or u may have to go so far in the past that it no longer apllies to present\n",
    "\n",
    "Can use lead score as way to leverage more plentiful session data - w/o relying on monetization events\n",
    "\n",
    "Can split `RPC_obs` into 2 factors and a bias term: `RPQ_obs`, `QPS_obs`, and `R0` (revenue at 0 quality).  Quality is an abstract unit - the idea is that is is derived from lead score and roughly linear w/ revenue on a session level basis.  Ideally - we want to be able to calculate quality w/in sql and aggregate it at query time so we dont have to work w/ session level data.  `QPS_obs` can be aggregated w/in a modifier bucket - using plentiful session data.  `RPQ_obs` can be aggregated accross the dataset - under the assumption that `RPQ_obs` will be stable accross an entire channel/platform/product - and possibly b/w platforms/products\n",
    "```\n",
    "RPC_obs = (RPQ_obs * QPS_obs + R0)\n",
    "CPC_target = RPC_obs / ROI_target = (RPQ_obs * QPS_obs + R0) / ROI_target\n",
    "```\n",
    "\n",
    "### Domain\n",
    "Biddable Dimensions:\n",
    "1. Location (DMA)\n",
    "2. Operating System\n",
    "3. Device Type\n",
    "4. Publisher (website ad was served on)\n",
    "5. Time of Day\n",
    "\n",
    "Split will most likely be on `(DMA,OS,Device)`.  Unsure if there will be enough session level data to support splitting on dimensions beyond that.  We might have to consider splits in isolation.  Alternately - we could try using a k-means or percentile-based clustering method to group our data.\n",
    "\n",
    "### Evaluation\n",
    "Bascially we want this `rps` estimation to capture long term trends w/o \n",
    "\n",
    "\n",
    "### Initial approach\n",
    "Will just naively use `lead_score` as a standin for quality - I think that ultimately precision at that lead score would be the best quality metric - since it basically corresponds to observed conversion rate.  But I think that is best accomplished on @sperks side.  Lets see how this goes first.\n",
    "\n",
    "For a given split - I will pull in revenue averages, bucket size, and lead score averages for each bucket.  Will compute `RPQ_obs` and `R0` by fitting a 1 var regressor w/ `y=rps_avg` and `X=[lead_score_avg]` - will use `bucket size` as sample weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load environ from: `SM_ENV_BASE`\n",
      "...Success!!\n"
     ]
    }
   ],
   "source": [
    "from utils.env import load_env_from_aws\n",
    "load_env_from_aws()\n",
    "\n",
    "\n",
    "from ds_utils.db.connectors import HealthcareDW\n",
    "from notebooks.aduriseti_shared.utils import *\n",
    "\n",
    "import functools\n",
    "import datetime\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "start_date = NOW-30*DAY\n",
    "end_date = NOW\n",
    "product=None\n",
    "traffic_source = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:query: SELECT r.topic, r.content_type, r.exchange, r.received, r.bo... executed in 195.33 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>content_type</th>\n      <th>exchange</th>\n      <th>received</th>\n      <th>sessionid</th>\n      <th>computed_dt</th>\n      <th>jornayaid</th>\n      <th>score</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619787520</td>\n      <td>20210430125806.496a01d470ff</td>\n      <td>2021-04-30</td>\n      <td>A0FD2AB6-7360-9C96-4E44-73569DE84C7B</td>\n      <td>0.0059</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619787588</td>\n      <td>20210430125655.750c001ba9e8</td>\n      <td>2021-04-30</td>\n      <td>C3C2480E-0944-D117-6B2F-1CDABCCE6FB1</td>\n      <td>0.0042</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619785495</td>\n      <td>20210430122415.c4cca73192a2</td>\n      <td>2021-04-30</td>\n      <td>00E7AC41-A7D5-5A65-01AE-B5ECFD732EC4</td>\n      <td>0.0355</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619784719</td>\n      <td>20210430121108.94a2dc19f0e6</td>\n      <td>2021-04-30</td>\n      <td>652B0816-1B5A-D83F-5E9A-06BD053EB7FA</td>\n      <td>0.0193</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619784418</td>\n      <td>20210430120604.d7fd836f5638</td>\n      <td>2021-04-30</td>\n      <td>C64870C5-10B6-62FA-018D-F4DE5DA2167D</td>\n      <td>0.0398</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619786911</td>\n      <td>20210430124528.272236209c6a</td>\n      <td>2021-04-30</td>\n      <td>F3006EF9-762D-AEC5-D70C-BDEC018CF073</td>\n      <td>0.0246</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619784564</td>\n      <td>20210430120637.884c31a7a914</td>\n      <td>2021-04-30</td>\n      <td>1567327E-E0CE-2C87-6F03-D823EAA34334</td>\n      <td>0.0061</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619786911</td>\n      <td>20210430124528.272236209c6a</td>\n      <td>2021-04-30</td>\n      <td>F3006EF9-762D-AEC5-D70C-BDEC018CF073</td>\n      <td>0.0246</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619784975</td>\n      <td>20210430121417.f1ed2c2470c8</td>\n      <td>2021-04-30</td>\n      <td>8950F5A5-5250-34B7-A8E7-18694C79CFE8</td>\n      <td>0.0237</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>healthcare.lead.score.scored</td>\n      <td></td>\n      <td>kraken</td>\n      <td>1619786209</td>\n      <td>20210430123338.a2a769922678</td>\n      <td>2021-04-30</td>\n      <td>7AFDA616-72A1-FFB3-FAF6-348DD5725EF7</td>\n      <td>0.0139</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 9 columns</p>\n</div>",
      "text/plain": "                           topic content_type exchange    received  \\\n0   healthcare.lead.score.scored                kraken  1619787520   \n1   healthcare.lead.score.scored                kraken  1619787588   \n2   healthcare.lead.score.scored                kraken  1619785495   \n3   healthcare.lead.score.scored                kraken  1619784719   \n4   healthcare.lead.score.scored                kraken  1619784418   \n..                           ...          ...      ...         ...   \n95  healthcare.lead.score.scored                kraken  1619786911   \n96  healthcare.lead.score.scored                kraken  1619784564   \n97  healthcare.lead.score.scored                kraken  1619786911   \n98  healthcare.lead.score.scored                kraken  1619784975   \n99  healthcare.lead.score.scored                kraken  1619786209   \n\n                      sessionid computed_dt  \\\n0   20210430125806.496a01d470ff  2021-04-30   \n1   20210430125655.750c001ba9e8  2021-04-30   \n2   20210430122415.c4cca73192a2  2021-04-30   \n3   20210430121108.94a2dc19f0e6  2021-04-30   \n4   20210430120604.d7fd836f5638  2021-04-30   \n..                          ...         ...   \n95  20210430124528.272236209c6a  2021-04-30   \n96  20210430120637.884c31a7a914  2021-04-30   \n97  20210430124528.272236209c6a  2021-04-30   \n98  20210430121417.f1ed2c2470c8  2021-04-30   \n99  20210430123338.a2a769922678  2021-04-30   \n\n                               jornayaid   score model  \n0   A0FD2AB6-7360-9C96-4E44-73569DE84C7B  0.0059  None  \n1   C3C2480E-0944-D117-6B2F-1CDABCCE6FB1  0.0042  None  \n2   00E7AC41-A7D5-5A65-01AE-B5ECFD732EC4  0.0355  None  \n3   652B0816-1B5A-D83F-5E9A-06BD053EB7FA  0.0193  None  \n4   C64870C5-10B6-62FA-018D-F4DE5DA2167D  0.0398  None  \n..                                   ...     ...   ...  \n95  F3006EF9-762D-AEC5-D70C-BDEC018CF073  0.0246  None  \n96  1567327E-E0CE-2C87-6F03-D823EAA34334  0.0061  None  \n97  F3006EF9-762D-AEC5-D70C-BDEC018CF073  0.0246  None  \n98  8950F5A5-5250-34B7-A8E7-18694C79CFE8  0.0237  None  \n99  7AFDA616-72A1-FFB3-FAF6-348DD5725EF7  0.0139  None  \n\n[100 rows x 9 columns]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with HealthcareDW() as db:\n",
    "    # df = db.to_df(f\"\"\"\n",
    "    #     SELECT\n",
    "    #         r.topic,\n",
    "    #         r.content_type,\n",
    "    #         r.exchange,\n",
    "    #         r.received,\n",
    "    #         r.body.sessionid,\n",
    "    #         r.body.\"on\"                 AS computed_ts,\n",
    "    #         r.body.jornayaid,\n",
    "    #         r.body.score,\n",
    "    #         r.body.response.meta.model\n",
    "    #     FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "    #     WHERE\n",
    "    #         {start_date} <= r.year AND r.year <= {end_date.year}\n",
    "    #         and {start_date.month} <= r.month AND r.month < {end_date.month}\n",
    "    #     LIMIT 100\n",
    "    # \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    # df = db.to_df(f\"\"\"\n",
    "    #     SELECT\n",
    "    #         r.topic,\n",
    "    #         r.content_type,\n",
    "    #         r.exchange,\n",
    "    #         r.received,\n",
    "    #         r.body.sessionid,\n",
    "    #         TO_TIMESTAMP(r.body.\"on\", 'YYYY-MM-DD HH24:MM:SS') \n",
    "    #                                     AS computed_ts,\n",
    "    #         r.body.jornayaid,\n",
    "    #         r.body.score,\n",
    "    #         r.body.response.meta.model\n",
    "    #     FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "    #     WHERE\n",
    "    #         {start_date} <= computed_ts AND computed_ts <= {end_date}\n",
    "    #     LIMIT 100\n",
    "    # \"\"\")\n",
    "\n",
    "    # df = db.to_df(f\"\"\"\n",
    "    #     SELECT\n",
    "    #         r.topic,\n",
    "    #         r.content_type,\n",
    "    #         r.exchange,\n",
    "    #         r.received,\n",
    "    #         r.body.sessionid,\n",
    "    #         TO_DATE(r.body.\"on\", 'YYYY-MM-DD')      AS computed_dt,\n",
    "    #         r.body.jornayaid,\n",
    "    #         r.body.score,\n",
    "    #         r.body.response.meta.model\n",
    "    #     FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "    #     WHERE\n",
    "    #         ({start_date.year} < r.year OR ({start_date.year} <= r.year AND {start_date.month} <= r.month)) AND\n",
    "    #         (r.year < {end_date.year} OR (r.year <= {end_date.year} AND r.month <= {end_date.month})) AND\n",
    "    #         {start_date.date()} <= computed_dt AND computed_dt <= {end_date.date()}\n",
    "    #     LIMIT 100\n",
    "    # \"\"\")\n",
    "\n",
    "    df = db.to_df(f\"\"\"\n",
    "        SELECT\n",
    "            r.topic,\n",
    "            r.content_type,\n",
    "            r.exchange,\n",
    "            r.received,\n",
    "            r.body.sessionid,\n",
    "            TO_DATE(r.body.\"on\", 'YYYY-MM-DD')      AS computed_dt,\n",
    "            r.body.jornayaid,\n",
    "            r.body.score,\n",
    "            r.body.response.meta.model\n",
    "        FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "        LIMIT 100\n",
    "    \"\"\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_filter = \"\" if product is None else \\\n",
    "    f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "traffic_filter = \"\" if traffic_source is None else \\\n",
    "    f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "session_revenue_sql = f\"\"\"\n",
    "SELECT\n",
    "    session_id,\n",
    "    sum(revenue) AS revenue\n",
    "FROM tron.session_revenue\n",
    "WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "    {product_filter}\n",
    "    {traffic_filter}\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "geoip_sql = f\"\"\"\n",
    "SELECT \n",
    "    l.*,\n",
    "    b.netowrk_index,\n",
    "    b.start_int,\n",
    "    b.end_int\n",
    "FROM \n",
    "    data_science.maxmind_ipv4_geo_blocks AS b\n",
    "    JOIN data_science.maxmind_geo_locations AS l\n",
    "        ON b.maxmind_id = l.maxmind_id\n",
    "\"\"\"\n",
    "sql_query = f\"\"\"\n",
    "with\n",
    "    rps as ({session_revenue_sql}),\n",
    "    ip_locs as ({geoip_sql}),\n",
    "    rps_tz_adj as (\n",
    "        SELECT\n",
    "            s.creation_date                                         AS utc_ts,\n",
    "            extract(\n",
    "                HOUR FROM\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                    - s.creation_date\n",
    "            )::INT                                                  AS utc_offset,\n",
    "            l.time_zone,\n",
    "            convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "            date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "            date_part(HOUR, user_ts) +\n",
    "            CASE \n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "            END                                                     AS hourofday,\n",
    "            l.subdivision_1_iso_code                                AS state,\n",
    "            l.metro_code                                            AS dma,\n",
    "            r.revenue\n",
    "        FROM \n",
    "            tracking.session_detail AS s\n",
    "            JOIN ip_locs as l\n",
    "                ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                AND l.country_iso_code = 'US'\n",
    "            INNER JOIN rps as r\n",
    "                ON s.session_id = r.session_id\n",
    "            LEFT JOIN \n",
    "        WHERE nullif(s.ip_address, '') IS NOT null\n",
    "            AND nullif(dma,'') IS NOT NULL \n",
    "            AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "    )\n",
    "SELECT\n",
    "    *\n",
    "FROM \n",
    "    rps_tz_adj\n",
    "LIMIT 100\n",
    ";\n",
    "\"\"\"\n",
    "with HealthcareDW() as db:\n",
    "    session_rps_df = df.to_df(sql_query)\n",
    "session_rps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_filter = \"\" if product is None else \\\n",
    "    f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "traffic_filter = \"\" if traffic_source is None else \\\n",
    "    f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "agg_rps_query = f\"\"\"\n",
    "with\n",
    "    rps as (\n",
    "        SELECT\n",
    "            session_id,\n",
    "            sum(revenue) AS revenue\n",
    "        FROM tron.session_revenue\n",
    "        WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "        GROUP BY 1\n",
    "    ),\n",
    "    ip_locs as (\n",
    "        SELECT \n",
    "            l.*,\n",
    "            b.netowrk_index,\n",
    "            b.start_int,\n",
    "            b.end_int\n",
    "        FROM \n",
    "            data_science.maxmind_ipv4_geo_blocks AS b\n",
    "            JOIN data_science.maxmind_geo_locations AS l\n",
    "                ON b.maxmind_id = l.maxmind_id\n",
    "    ),\n",
    "    rps_tz_adj as (\n",
    "        SELECT\n",
    "            s.creation_date                                         AS utc_ts,\n",
    "            extract(\n",
    "                HOUR FROM\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                    - s.creation_date\n",
    "            )::INT                                                  AS utc_offset,\n",
    "            l.time_zone,\n",
    "            convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "            date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "            date_part(HOUR, user_ts) +\n",
    "            CASE \n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "            END                                                     AS hourofday,\n",
    "            l.subdivision_1_iso_code                                AS state,\n",
    "            l.metro_code                                            AS dma,\n",
    "            r.revenue\n",
    "        FROM \n",
    "            tracking.session_detail AS s\n",
    "            JOIN ip_locs as l\n",
    "                ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                AND l.country_iso_code = 'US'\n",
    "            INNER JOIN rps as r\n",
    "                ON s.session_id = r.session_id\n",
    "        WHERE nullif(s.ip_address, '') IS NOT null\n",
    "            AND nullif(dma,'') IS NOT NULL \n",
    "            AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "    )\n",
    "SELECT\n",
    "    {','.join(agg_columns)},\n",
    "    COUNT(session_id)                                                       AS sessions,\n",
    "    SUM((revenue>0)::INT::FLOAT)                                            AS num_leads,\n",
    "    AVG((revenue>0)::INT::FLOAT)                                            AS lps_avg,\n",
    "    SUM(revenue) / CASE\n",
    "        WHEN num_leads = 0 THEN 1\n",
    "        ELSE num_leads\n",
    "    END                                                                     AS rpl_avg,\n",
    "    (SUM(revenue) / COUNT(DISTINCT session_id))::NUMERIC(8,4)               AS rps_,\n",
    "    AVG(revenue)                                                            AS rps_avg,\n",
    "    STDDEV(revenue)                                                         AS rps_std,\n",
    "    VARIANCE(revenue)                                                       AS rps_var\n",
    "FROM rps_tz_adj\n",
    "GROUP BY {','.join(agg_columns)}\n",
    "\"\"\"\n",
    "# print(agg_rps_query)\n",
    "# print(traffic_filter)\n",
    "from ds_utils.db.connectors import HealthcareDW\n",
    "with HealthcareDW() as db:\n",
    "    df = db.to_df(agg_rps_query)\n",
    "globals()[\"df\"] = df\n",
    "\n",
    "delt = df[\"rps_avg\"] - df['rps_']\n",
    "if not all(delt.abs() < 1e-3):\n",
    "    print(\"session uniqueness assummption not satisfied\")\n",
    "df = df \\\n",
    "    .sort_values(by=agg_columns, ascending=True) \\\n",
    "    .set_index(agg_columns)\n",
    "\n",
    "df['int_ix'] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "import functools\n",
    "@functools.lru_cache()\n",
    "def agg_rps(start_date,end_date,product,traffic_source,agg_columns):\n",
    "    agg_columns = list(agg_columns)\n",
    "    product_filter = \"\" if product is None else \\\n",
    "        f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "    traffic_filter = \"\" if traffic_source is None else \\\n",
    "        f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "    agg_rps_query = f\"\"\"\n",
    "    with\n",
    "        rps as (\n",
    "            SELECT\n",
    "                session_id,\n",
    "                sum(revenue) AS revenue\n",
    "            FROM tron.session_revenue\n",
    "            WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "                {product_filter}\n",
    "                {traffic_filter}\n",
    "            GROUP BY 1\n",
    "        ),\n",
    "        ip_locs as (\n",
    "            SELECT \n",
    "                l.*,\n",
    "                b.netowrk_index,\n",
    "                b.start_int,\n",
    "                b.end_int\n",
    "            FROM \n",
    "                data_science.maxmind_ipv4_geo_blocks AS b\n",
    "                JOIN data_science.maxmind_geo_locations AS l\n",
    "                    ON b.maxmind_id = l.maxmind_id\n",
    "        ),\n",
    "        rps_tz_adj as (\n",
    "            SELECT\n",
    "                s.*,\n",
    "                s.creation_date                                         AS utc_ts,\n",
    "                extract(\n",
    "                    HOUR FROM\n",
    "                    convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                        - s.creation_date\n",
    "                )::INT                                                  AS utc_offset,\n",
    "                l.time_zone,\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "                date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "                date_part(HOUR, user_ts) +\n",
    "                CASE \n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "                END                                                     AS hourofday,\n",
    "                l.subdivision_1_iso_code                                AS state,\n",
    "                l.metro_code                                            AS dma,\n",
    "                r.revenue\n",
    "            FROM \n",
    "                tracking.session_detail AS s\n",
    "                JOIN ip_locs as l\n",
    "                    ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                    AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                    AND l.country_iso_code = 'US'\n",
    "                INNER JOIN rps as r\n",
    "                    ON s.session_id = r.session_id\n",
    "            WHERE nullif(s.ip_address, '') IS NOT null\n",
    "                AND nullif(dma,'') IS NOT NULL \n",
    "                AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "                {product_filter}\n",
    "                {traffic_filter}\n",
    "        )\n",
    "    SELECT\n",
    "        {','.join(agg_columns)},\n",
    "        COUNT(session_id)                                                       AS sessions,\n",
    "        SUM((revenue>0)::INT::FLOAT)                                            AS num_leads,\n",
    "        AVG((revenue>0)::INT::FLOAT)                                            AS lps_avg,\n",
    "        SUM(revenue) / CASE\n",
    "            WHEN num_leads = 0 THEN 1\n",
    "            ELSE num_leads\n",
    "        END                                                                     AS rpl_avg,\n",
    "        (SUM(revenue) / COUNT(DISTINCT session_id))::NUMERIC(8,4)               AS rps_,\n",
    "        AVG(revenue)                                                            AS rps_avg,\n",
    "        STDDEV(revenue)                                                         AS rps_std,\n",
    "        VARIANCE(revenue)                                                       AS rps_var\n",
    "    FROM rps_tz_adj\n",
    "    GROUP BY {','.join(agg_columns)}\n",
    "    \"\"\"\n",
    "    # print(agg_rps_query)\n",
    "    # print(traffic_filter)\n",
    "    from ds_utils.db.connectors import HealthcareDW\n",
    "    with HealthcareDW() as db:\n",
    "        df = db.to_df(agg_rps_query)\n",
    "    globals()[\"df\"] = df\n",
    "\n",
    "    delt = df[\"rps_avg\"] - df['rps_']\n",
    "    if not all(delt.abs() < 1e-3):\n",
    "        print(\"session uniqueness assummption not satisfied\")\n",
    "    df = df \\\n",
    "        .sort_values(by=agg_columns, ascending=True) \\\n",
    "        .set_index(agg_columns)\n",
    "\n",
    "    df['int_ix'] = range(len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "campaign_split_fields = dict(\n",
    "    # traffic_source = [\"traffic_source\"],\n",
    "    browser = [\"browser\"],\n",
    "    operating_system = [\"operating_system\"],\n",
    "    device = [\"device\"],\n",
    "    # channel = [\"channel\"],\n",
    "    # domain = [\"domain\"],\n",
    "    product = [\"product\"],\n",
    "    # keyword = [\"keyword\"],\n",
    "    # campaign_id = [\"campaign_id\"],\n",
    "    # landing_page = [\"landing_page\"],\n",
    "    TOD = [\"dayofweek\",\"hourofday\"],\n",
    "    dma = [\"dma\"],\n",
    "    state =[\"state\",],\n",
    "    location = [\"state\",\"dma\"],\n",
    "    \n",
    "    dma_os=[\"dma\", \"operating_system\"],\n",
    "    dma_device=[\"dma\", \"device\", ],\n",
    "    dma_os_device=[\"dma\", \"operating_system\", \"device\"],\n",
    "\n",
    "    state_os=[\"state\", \"operating_system\"],\n",
    "    state_device=[\"state\", \"device\", ],\n",
    "    state_os_device=[\"state\", \"operating_system\", \"device\"],\n",
    "\n",
    "    location_os = [\"state\", \"dma\", \"operating_system\"],\n",
    "    location_device=[\"state\", \"dma\", \"device\", ],\n",
    "    location_os_device = [\"state\", \"dma\", \"operating_system\",\"device\"],\n",
    ")\n",
    "\n",
    "taboola_val_map = {\n",
    "    \"device\": {\n",
    "        'DESKTOP': 'DESK',\n",
    "        'MOBILE': 'PHON',\n",
    "        'TABLET': 'TBLT',\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        'Linux armv7l': \"Linux\",\n",
    "        'Linux armv8l': \"Linux\",\n",
    "        'Linux x86_64': \"Linux\",\n",
    "        'MacIntel': 'Mac OS X',\n",
    "        'Win32': \"Windows\",\n",
    "        'iPad': \"iPadOS\",\n",
    "        'iPhone': \"iOS\",\n",
    "        '': None,\n",
    "        'ARM': None,\n",
    "        'Android': 'Android',\n",
    "        'Linux aarch64': \"Linux\",\n",
    "        'Win64': \"Windows\",\n",
    "        'Linux armv7': \"Linux\",\n",
    "        'Linux i686': \"Linux\",\n",
    "        'Windows': \"Windows\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def translate_taboola_vals(df):\n",
    "    index_cols = df.index.names\n",
    "    df = df.reset_index()\n",
    "    for c in df.columns:\n",
    "        if c in taboola_val_map:\n",
    "            df[c] = df[c].apply(taboola_val_map[c].__getitem__)\n",
    "    df_bkp = df\n",
    "    df = df \\\n",
    "        .groupby(index_cols) \\\n",
    "        .agg({\n",
    "            \"sessions\": sum,\n",
    "            \"num_leads\": sum,\n",
    "            \"lps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rpl_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "        })\n",
    "    df[\"int_ix\"] = range(len(df))\n",
    "    df_bkp_wavg = wavg(df_bkp[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                        df_bkp[\"sessions\"].values.reshape(-1, 1))\n",
    "    df_wavg = wavg(df[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                    df[\"sessions\"].values.reshape(-1, 1))\n",
    "    assert all((df_bkp_wavg - df_wavg).abs() < 1e-2), (df_bkp_wavg,df_wavg)\n",
    "    return df\n",
    "\n",
    "import pprint\n",
    "from IPython.display import display as ipydisp    \n",
    "import pandas as pd\n",
    "from models.utils import wavg\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def wvar(V,W):\n",
    "    mu = wavg(V,W)\n",
    "    var = wavg((V - mu)**2,W)\n",
    "    return var\n",
    "def wstd(V,W):\n",
    "    return wvar(V,W)**0.5\n",
    "def get_wavg_by(df, col):\n",
    "    def wavg_by(V):\n",
    "        return wavg(V, W=df.loc[V.index, col])\n",
    "    return wavg_by\n",
    "\n",
    "def get_wthresh(W,p):\n",
    "    W = rps_df[\"sessions\"].sort_values(ascending=False)\n",
    "    Wsum = W.sum()\n",
    "    cumsum = 0\n",
    "    for wthresh in W:\n",
    "        if cumsum > Wsum * p:\n",
    "            break\n",
    "        cumsum += wthresh\n",
    "    return wthresh\n",
    "\n",
    "\"\"\"\n",
    "TODO: 2021-05-20\n",
    "  - test fitting on general traffic sources?\n",
    "  - test clustering on multiple days of rps data - \n",
    "      or rolling rps data or something\n",
    "  - try out more granular aggs\n",
    "  - is there some kind of metric which measures total variance\n",
    "      and computes how much of that variance is captured by a split?\n",
    "\n",
    "Trevor: 2021-05-21\n",
    "TODO:\n",
    "- figure out how to minimize campaign # when writing back to taboola\n",
    "    - want to make sure campaigns have sufficient traffic\n",
    "- kw=(location,os,device)\n",
    "- calc 30-day rps/kw\n",
    "- can create distribution over the rps(kw) distribution\n",
    "- 100 campaigns - 1 per percentile\n",
    "\n",
    "TODO: 2021-05-24\n",
    "- look into clustering each individual split variable\n",
    "\"\"\"\n",
    "\n",
    "# split2aggrps = {}\n",
    "# for split,split_cols in campaign_split_fields.items():\n",
    "#     print(split,split_cols)\n",
    "#     rps_df = agg_rps(NOW-90*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=split_cols)\n",
    "#     rps_df = translate_taboola_vals(rps_df)\n",
    "#     rps_df[\"split_on\"] = split\n",
    "#     split2aggrps[split] = rps_df\n",
    "#     print(split,rps_df.shape)\n",
    "\n",
    "\n",
    "def agg_rps(start_date, end_date, product, traffic_source, agg_columns):\n",
    "\n",
    "\n",
    "def agg_rps_taboola(start_date, end_date, product, traffic_source, agg_columns):\n",
    "    rps_df = agg_rps(start_date,end_date,None,traffic_source=traffic_source,agg_columns=agg_columns)\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "\n",
    "split2fitaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-90*DAY,NOW-30*DAY,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2fitaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n",
    "\n",
    "split2evalaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-30*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2evalaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.cluster\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "def cluster_split(split):\n",
    "    rps_df_fit = split2fitaggrps[split]\n",
    "    rps_df_eval = split2evalaggrps[split]\n",
    "    split_size = rps_df_fit.__len__()\n",
    "    MINCLUST = 8\n",
    "    if split_size <= MINCLUST:\n",
    "        rps_clust_df = rps_df_eval\n",
    "        rps_df_fit[\"clust\"] = 0\n",
    "        rps_df_eval[\"clust\"] = 0\n",
    "    else:\n",
    "        # nclust = MINCLUST\n",
    "        nclust = max(MINCLUST, int(np.log(split_size)))\n",
    "        # nclust = max(MINCLUST,int(split_size ** 0.5))\n",
    "        # print(\"nclust\", nclust, split_size, np.log(split_size))\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=nclust)\n",
    "        rps_df_fit[\"clust\"] = clusterer \\\n",
    "            .fit_predict(\n",
    "                rps_df_fit[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_fit[\"sessions\"])\n",
    "        rps_df_eval[\"clust\"] = clusterer \\\n",
    "            .predict(\n",
    "                rps_df_eval[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_eval[\"sessions\"])\n",
    "        rps_clust_df = rps_df_eval \\\n",
    "            .groupby(\"clust\") \\\n",
    "            .agg({\"rps_avg\": get_wavg_by(rps_df_eval, \"sessions\"), \"sessions\": sum})\n",
    "    assert rps_clust_df[\"rps_avg\"].max() <= rps_df_eval[\"rps_avg\"].max()\n",
    "    rps_wavg = wavg(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"])\n",
    "    rps_clust_wavg = wavg(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"])\n",
    "    assert abs(rps_wavg - rps_clust_wavg) < 1e-4, (rps_wavg, rps_clust_wavg)\n",
    "    return rps_df_fit,rps_df_eval,rps_clust_df\n",
    "\n",
    "def get_split_factor(rps_df):\n",
    "    orig_index = rps_df.index.names\n",
    "    split_attr2unique_vals = {index_col: rps_df.index.unique(\n",
    "        index_col) for index_col in orig_index}\n",
    "    _,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "    nclusts = rps_df[\"clust\"].unique().__len__()\n",
    "    split_factor = nclusts * np.prod([1] + [split_attr2unique_vals[c].__len__() for c in new_index_order[:-1]])\n",
    "    return split_factor\n",
    "\n",
    "perfD = []\n",
    "for split in campaign_split_fields.keys():\n",
    "    rps_df_fit,rps_df_eval,rps_clust_df = cluster_split(split)\n",
    "    perfd = {\n",
    "        \"split\": split,\n",
    "        \"fit_shape\": rps_df_fit.shape,\n",
    "        \"clust_shape\": rps_clust_df.shape,\n",
    "        # wavg(rps_df[\"rps_avg\"],rps_df[\"sessions\"]),\n",
    "        \"split_variance\": wstd(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"]),\n",
    "        \"cluster_variance\": wstd(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"]),\n",
    "        # wstd(rps_df[\"rps_avg\"],rps_df[\"sessions\"])\n",
    "        \"split_factor\": rps_df_fit.__len__(),\n",
    "        \"clustered_split_factor\": get_split_factor(rps_df_fit),\n",
    "    }\n",
    "    perfD.append(perfd)\n",
    "    pprint.pprint(perfd)\n",
    "    ipydisp(rps_clust_df)\n",
    "\n",
    "perfdf = pd.DataFrame(perfD)\n",
    "ipydisp(perfdf)\n",
    "#%%\n",
    "# BEST_SPLIT = \"location_os_device\"\n",
    "BEST_SPLIT = \"dma_os_device\"\n",
    "# BEST_SPLIT = \"dma_os\"\n",
    "\n",
    "rps_df_fit, rps_df_eval, rps_clust_df = cluster_split(BEST_SPLIT)\n",
    "rps_df = rps_df_fit\n",
    "get_split_factor(rps_df)\n",
    "orig_index = rps_df.index.names\n",
    "split_attr2unique_vals = {index_col: rps_df.index.unique(index_col) for index_col in orig_index}\n",
    "_,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "rps_df = rps_df .reset_index()\n",
    "campaign_df = rps_df \\\n",
    "    .groupby([*new_index_order[:-1], \"clust\"]) \\\n",
    "    .agg({\n",
    "        \"sessions\": sum,\n",
    "        \"rps_avg\": get_wavg_by(rps_df,\"sessions\"),\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "assert campaign_df[\"sessions\"].sum() == rps_df_fit[\"sessions\"].sum()\n",
    "camp_rps_wavg = wavg(campaign_df[\"rps_avg\"],campaign_df[\"sessions\"])\n",
    "fit_rps_wavg = wavg(rps_df_fit[\"rps_avg\"], rps_df_fit[\"sessions\"])\n",
    "assert abs(camp_rps_wavg - fit_rps_wavg) < 1e-5\n",
    "\n",
    "excl_campaign_df = campaign_df.groupby([*new_index_order[:-1]]) \\\n",
    "    .agg({\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "def flatten(M):\n",
    "    return tuple(el for r in M for el in r)\n",
    "excl_campaign_df[new_index_order[-1]] = excl_campaign_df[new_index_order[-1]] .apply(flatten)\n",
    "\n",
    "camps = []\n",
    "for idx,r in campaign_df.iterrows():\n",
    "    camp = {\n",
    "        \"sessions_60d\": r[\"sessions\"], \n",
    "        \"rps_avg_60d\": r[\"rps_avg\"]\n",
    "    }\n",
    "    for field,val in zip(new_index_order[:-1],idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"includes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "for idx, r in excl_campaign_df.iterrows():\n",
    "    camp = {}\n",
    "    for field, val in zip(new_index_order[:-1], idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"excludes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "camp_df = pd.DataFrame(camps)\n",
    "camp_df.to_csv(\"campaign_dump.csv\")\n",
    "#%%\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn.feature_selection\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "\n",
    "requires_campaign_split = [\n",
    "    # \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    # \"domain\",\n",
    "    \"product\",\n",
    "    # \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "\n",
    "for split in requires_campaign_split:\n",
    "    rps_df = split2aggrps[split] .reset_index()\n",
    "    Xs = rps_df \\\n",
    "        .apply(lambda r: [r[\"int_ix\"]]*int(r[\"sessions\"]), axis=1)\n",
    "    X = np.concatenate(Xs.values).reshape(-1,1)\n",
    "    ys = rps_df \\\n",
    "        .apply(lambda r: [r[\"rps_avg\"]]*int(r[\"sessions\"]),axis=1)\n",
    "    y = np.concatenate(ys.values)\n",
    "    # print(y.min(),np.quantile(y, 0.5),y.max())\n",
    "    y = y > y.mean()\n",
    "    # y = np.concatenate(ys.values).reshape(-1,1)\n",
    "    # y = sklearn.preprocessing.KBinsDiscretizer(n_bins=2,encode=\"ordinal\") \\\n",
    "    #     .fit_transform(y).reshape(-1)\n",
    "    mi = sklearn.feature_selection.mutual_info_regression(X,y,discrete_features=True)\n",
    "    print(split,mi,y.mean(),rps_df.shape)\n",
    "#%%\n",
    "y\n",
    "#%%\n",
    "sklearn.metrics.mutual_info_score\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "requires_campaign_split = [\n",
    "    \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    # \"domain\",\n",
    "    \"product\",\n",
    "    # \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "combined_rps_df = pd.concat([df.reset_index() for df in split2aggrps.values()])\n",
    "splitI = combined_rps_df[\"split_on\"].isin(requires_campaign_split)\n",
    "density = scipy.stats.gaussian_kde(\n",
    "    dataset=combined_rps_df.loc[splitI,\"rps_avg\"],\n",
    "    weights=combined_rps_df.loc[splitI, \"sessions\"],\n",
    "    # bw_method=\"scott\",\n",
    "    # bw_method=\"silverman\",\n",
    "    # bw_method=0.1,\n",
    ")\n",
    "xs = np.linspace(0,2,100)\n",
    "plt.plot(xs,density(xs))\n",
    "#%%\n",
    "for split in requires_campaign_split:\n",
    "    splitI = combined_rps_df[\"split_on\"] == split\n",
    "    density = scipy.stats.gaussian_kde(\n",
    "        dataset=combined_rps_df.loc[splitI, \"rps_avg\"],\n",
    "        weights=combined_rps_df.loc[splitI, \"sessions\"])\n",
    "    xs = np.linspace(0, 2, 100)\n",
    "    plt.plot(xs, density(xs))\n",
    "    plt.title(split)\n",
    "    plt.show()\n",
    "#%%\n",
    "combined_rps_df.loc[splitI, [\"rps_avg\",\"sessions\"]].apply(\n",
    "        lambda r: pd.Series([r[\"rps_avg\"]]*int(r[\"sessions\"])),axis=1) \\\n",
    "    .unstack()\n",
    "#%%\n",
    "agg_rps = split2aggrps[\"TOD\"]\n",
    "Xy = agg_rps[[\"sessions\",\"rps\"]].reset_index()\n",
    "Xy[\"i\"] = range(len(Xy))\n",
    "Xy\n",
    "#%%\n",
    "import sklearn.feature_selection\n",
    "sklearn.feature_selection.mutual_info_regression(\n",
    "    []\n",
    ")\n",
    "\n",
    "#%%\n",
    "\"\"\"\n",
    "- \n",
    "- overall goal:\n",
    "    - specific ROI targetting w/ minimal campaigns\n",
    "- what this means for accnt structure\n",
    "    - for many variables we must split campaigns to target ROI\n",
    "    - want to capture greatest amt of rps variation w/ \n",
    "        fewest # of campaigns\n",
    "- 2 ways of approaching this\n",
    "    1. minimize rps variation w/in a campaign\n",
    "        - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "        - tried this out a little - but the computation was intensive which made it slow going\n",
    "    2. maximize rps variation outside campaigns\n",
    "    - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "    - mostly focused on this approach\n",
    "\n",
    "- approaches I tried\n",
    "1. rank splits by their correlation/covaraince/MI w/ `agg_rps`\n",
    "    - tried ANOVA,pt.biserial,MI - had most success w/ MI\n",
    "    - was dificult to compute b/c couldnt find methods that accepted sample weight\n",
    "    - MI approach was promising but didnt go down that route\n",
    "2. rank splits by intra-split agg_rps variance\n",
    "    - had most success w/ this\n",
    "    - tested out an approahc where I cluster the split on rps - had good results\n",
    "    - think this is the mtd to use going forward\n",
    "    TODO:\n",
    "    - test fitting on general traffic sources?\n",
    "    - test clustering on multiple days of rps data - \n",
    "        or rolling rps data or something\n",
    "    - try out more granular aggs\n",
    "    - is there some kind of metric which measures total variance\n",
    "        and computes how much of that variance is captured by a split?\n",
    "\n",
    "3. fit decision tree on rps data \n",
    "    - MSE criterion is apparently the same as minimizng inter split variance\n",
    "    - computationally intensive\n",
    "    - not 100% clear how to go from tree to campaign structure\n",
    "\n",
    "\n",
    "- can do this by\n",
    "    1. choosing what vars or tuples of vars to split campaigns on\n",
    "    2. grouping similar buckets w/in those splits\n",
    "- want to measure correlation/dependence of categorical split vars w/ cont rps\n",
    "    - cat,cat metrics w/ binned rps\n",
    "        chi\n",
    "    - cont,cont metrcs w/ 1 hot encoded split vars\n",
    "    - cat,cont metrics\n",
    "- correlation metrics\n",
    "    - MI:\n",
    "        - sklearn.feature_selection.mi_regression\n",
    "        - would need AFAICT to rresample input arrays\n",
    "    - ANOVA:\n",
    "        - scipy.stats.f_oneway\n",
    "    - pt biserial\n",
    "        - needs binary vars tho\n",
    "        - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html\n",
    "        - https://towardsdatascience.com/point-biserial-correlation-with-python-f7cd591bd3b1\n",
    "- want most variation of rps w/ fewest camapaigns\n",
    "- dont want to group similar buckets together - think its too complicated\n",
    "\n",
    "\"\"\"\n",
    "#%%\n",
    "requires_campaign_split = [\n",
    "    \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    \"domain\",\n",
    "    \"product\",\n",
    "    \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "#%%\n",
    "\n",
    "traffic_source = TABOOLA\n",
    "with HealthcareDW() as db:\n",
    "    traffic_filter = \"\" if traffic_source is None else \\\n",
    "        f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "    sql = f\"\"\"\n",
    "    select\n",
    "        traffic_source,domain,count(*)\n",
    "    from tracking.session_detail\n",
    "    where True \n",
    "    {traffic_filter}\n",
    "    group by traffic_source,domain;\n",
    "    \"\"\"\n",
    "    df = db.to_df(sql).sort_values(\"count\")\n",
    "df\n",
    "#%%\n",
    "\n",
    "# %%\n",
    "with HealthcareDW() as db:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('hc': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ea8b8f99ad076bcb7b5f9c57bdb2fa146095f918442d2de0af3010fd7fbcc33f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}