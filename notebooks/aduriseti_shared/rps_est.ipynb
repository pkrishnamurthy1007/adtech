{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement:\n",
    "Gdoc: https://docs.google.com/document/d/1xYIV3xRNxTjUaw1fBkwB0ujJxUH6l7Gc1Mrskzny6JI/edit?usp=sharing\n",
    "\n",
    "Trying to track ROI target by adjusting cpc - assume that RPC doesnt change w/ cpc\n",
    "```\n",
    "ROI_obs = RPC_obs / CPC_obs\n",
    "ROI_target = RPC_obs / CPC_target\n",
    "ROI_obs * CPC_obs = ROI_target * CPC_target\n",
    "CPC_target = ROI_obs/ROI_target * CPC_obs = RPC_obs / ROI_target\n",
    "```\n",
    "Currently we calculate `RPC_obs` against real data - but monietization data may be spares - or u may have to go so far in the past that it no longer apllies to present\n",
    "\n",
    "Can use lead score as way to leverage more plentiful session data - w/o relying on monetization events\n",
    "\n",
    "Can split `RPC_obs` into 2 factors and a bias term: `RPQ_obs`, `QPS_obs`, and `R0` (revenue at 0 quality).  Quality is an abstract unit - the idea is that is is derived from lead score and roughly linear w/ revenue on a session level basis.  Ideally - we want to be able to calculate quality w/in sql and aggregate it at query time so we dont have to work w/ session level data.  `QPS_obs` can be aggregated w/in a modifier bucket - using plentiful session data.  `RPQ_obs` can be aggregated accross the dataset - under the assumption that `RPQ_obs` will be stable accross an entire channel/platform/product - and possibly b/w platforms/products\n",
    "```\n",
    "RPC_obs = (RPQ_obs * QPS_obs + R0)\n",
    "CPC_target = RPC_obs / ROI_target = (RPQ_obs * QPS_obs + R0) / ROI_target\n",
    "```\n",
    "\n",
    "### Domain\n",
    "Biddable Dimensions:\n",
    "1. Location (DMA)\n",
    "2. Operating System\n",
    "3. Device Type\n",
    "4. Publisher (website ad was served on)\n",
    "5. Time of Day\n",
    "\n",
    "Split will most likely be on `(DMA,OS,Device)`.  Unsure if there will be enough session level data to support splitting on dimensions beyond that.  We might have to consider some split variables in isolation.  Alternately - we could try using a k-means or percentile-based clustering method to group our data w/in a very fine grained split.\n",
    "\n",
    "### Evaluation\n",
    "Bascially we want this `rps` estimation to capture long term trends w/o short term  noise.  Ideas:\n",
    "1. plot out rolling 1,7,14,30,60,90 day estimates of rps using regular aggregations and using the lead score stabilized rps estimate\n",
    "    - want time plots and histograms\n",
    "1. compare the rps esimation to rolling rps w/ large window\n",
    "    - at that pt - why not use that rolling rps to estimate?\n",
    "1. if the idea is that we are compensating for data deficiency - why dont we use large buckets to measure bias - and small buckets to measure variance - than combine into some metric?\n",
    "\n",
    "### Initial approach\n",
    "Will just naively use `lead_score` as a standin for quality - I think that ultimately precision at that lead score would be the best quality metric - since it basically corresponds to observed conversion rate.  But I think that is best accomplished on @sperks side.  Lets see how this goes first.\n",
    "\n",
    "For a given split - I will pull in revenue averages, bucket size, and lead score averages for each bucket.  Will compute `RPQ_obs` and `R0` by fitting a 1 var regressor w/ `y=rps_avg` and `X=[lead_score_avg]` - will use `bucket size` as sample weight.\n",
    "\n",
    "NOTE: i guess there are actually 2 differnt model scores per lead - I will pivot those out and the linear regressor will have 2 weights - 1 for each model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load environ from: `SM_ENV_BASE`\n",
      "...Success!!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "# detect if we are running from a `notebooks/*_shared` folder\n",
    "# re.match(\"not.*shared\",sys.path[0])\n",
    "if sys.path[0].endswith(\"_shared\"):\n",
    "    sys.path[0] = \"/\".join(sys.path[0].split(\"/\")[:-2])\n",
    "assert sys.path[0].endswith(\"adtech\")\n",
    "\n",
    "from utils.env import load_env_from_aws\n",
    "load_env_from_aws()\n",
    "\n",
    "\n",
    "from ds_utils.db.connectors import HealthcareDW\n",
    "from notebooks.aduriseti_shared.utils import *\n",
    "\n",
    "import functools\n",
    "import datetime\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "# start_date = NOW-30*DAY\n",
    "# end_date = NOW\n",
    "start_date = NOW-60*DAY\n",
    "end_date = NOW-30*DAY\n",
    "product=None\n",
    "traffic_source = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:query: SELECT r.topic, r.content_type, r.exchange, r.received, r.bo... executed in 45.37 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>content_type</th>\n",
       "      <th>exchange</th>\n",
       "      <th>received</th>\n",
       "      <th>sessionid</th>\n",
       "      <th>computed_dt</th>\n",
       "      <th>jornayaid</th>\n",
       "      <th>score</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1615496584</td>\n",
       "      <td>20210311210230.ddf2034a9e4d</td>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>741FC6DF-317B-4C04-1C09-69E4A4E15363</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1615497215</td>\n",
       "      <td>20210311211248.3fbbd3e62dca</td>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>A1D8122E-FA95-51E4-0FA6-0C9DAD3C9A4E</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1615497192</td>\n",
       "      <td>20210311211034.e54cfab8e245</td>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>6FE58484-4472-8873-22F3-6DA136451E1C</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1615497507</td>\n",
       "      <td>20210311211712.6076e20684ac</td>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>AC77A3A4-B4B8-42A4-75FE-A9129F313131</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1615498314</td>\n",
       "      <td>20210311212839.cd461c53ea8b</td>\n",
       "      <td>2021-03-11</td>\n",
       "      <td>698A7FA1-692C-993C-B144-F3D685FAD42C</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1619786581</td>\n",
       "      <td>20210430124100.6050630971aa</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>BC45395F-1C5D-51EA-6B92-915FEED86C8E</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1619784119</td>\n",
       "      <td>20210430120034.226b58a19852</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>89D17EAF-71F9-F20B-7CAA-511902004D26</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1619785987</td>\n",
       "      <td>20210430123022.1cbdc2054989</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>EAC7A1FD-D095-1F1B-8832-8ABDC5BA5D5D</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1619784485</td>\n",
       "      <td>20210430120644.e1e8f96f0b92</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>8EE46E95-BA1B-1123-D521-EB76E0CC6F0B</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>healthcare.lead.score.scored</td>\n",
       "      <td></td>\n",
       "      <td>kraken</td>\n",
       "      <td>1619786278</td>\n",
       "      <td>20210430123701.b7f86ddd6654</td>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>A9C23FD3-794D-449F-7E87-DB2CAE921F54</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           topic content_type exchange    received  \\\n",
       "0   healthcare.lead.score.scored                kraken  1615496584   \n",
       "1   healthcare.lead.score.scored                kraken  1615497215   \n",
       "2   healthcare.lead.score.scored                kraken  1615497192   \n",
       "3   healthcare.lead.score.scored                kraken  1615497507   \n",
       "4   healthcare.lead.score.scored                kraken  1615498314   \n",
       "..                           ...          ...      ...         ...   \n",
       "95  healthcare.lead.score.scored                kraken  1619786581   \n",
       "96  healthcare.lead.score.scored                kraken  1619784119   \n",
       "97  healthcare.lead.score.scored                kraken  1619785987   \n",
       "98  healthcare.lead.score.scored                kraken  1619784485   \n",
       "99  healthcare.lead.score.scored                kraken  1619786278   \n",
       "\n",
       "                      sessionid computed_dt  \\\n",
       "0   20210311210230.ddf2034a9e4d  2021-03-11   \n",
       "1   20210311211248.3fbbd3e62dca  2021-03-11   \n",
       "2   20210311211034.e54cfab8e245  2021-03-11   \n",
       "3   20210311211712.6076e20684ac  2021-03-11   \n",
       "4   20210311212839.cd461c53ea8b  2021-03-11   \n",
       "..                          ...         ...   \n",
       "95  20210430124100.6050630971aa  2021-04-30   \n",
       "96  20210430120034.226b58a19852  2021-04-30   \n",
       "97  20210430123022.1cbdc2054989  2021-04-30   \n",
       "98  20210430120644.e1e8f96f0b92  2021-04-30   \n",
       "99  20210430123701.b7f86ddd6654  2021-04-30   \n",
       "\n",
       "                               jornayaid   score model  \n",
       "0   741FC6DF-317B-4C04-1C09-69E4A4E15363  0.0195  None  \n",
       "1   A1D8122E-FA95-51E4-0FA6-0C9DAD3C9A4E  0.0000  None  \n",
       "2   6FE58484-4472-8873-22F3-6DA136451E1C  0.0405  None  \n",
       "3   AC77A3A4-B4B8-42A4-75FE-A9129F313131  0.0000  None  \n",
       "4   698A7FA1-692C-993C-B144-F3D685FAD42C  0.0000  None  \n",
       "..                                   ...     ...   ...  \n",
       "95  BC45395F-1C5D-51EA-6B92-915FEED86C8E  0.0063  None  \n",
       "96  89D17EAF-71F9-F20B-7CAA-511902004D26  0.0204  None  \n",
       "97  EAC7A1FD-D095-1F1B-8832-8ABDC5BA5D5D  0.0140  None  \n",
       "98  8EE46E95-BA1B-1123-D521-EB76E0CC6F0B  0.0230  None  \n",
       "99  A9C23FD3-794D-449F-7E87-DB2CAE921F54  0.0335  None  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with HealthcareDW() as db:\n",
    "    # df = db.to_df(f\"\"\"\n",
    "    #     SELECT\n",
    "    #         r.topic,\n",
    "    #         r.content_type,\n",
    "    #         r.exchange,\n",
    "    #         r.received,\n",
    "    #         r.body.sessionid,\n",
    "    #         r.body.\"on\"                 AS computed_ts,\n",
    "    #         r.body.jornayaid,\n",
    "    #         r.body.score,\n",
    "    #         r.body.response.meta.model\n",
    "    #     FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "    #     WHERE\n",
    "    #         {start_date} <= r.year AND r.year <= {end_date.year}\n",
    "    #         and {start_date.month} <= r.month AND r.month < {end_date.month}\n",
    "    #     LIMIT 100\n",
    "    # \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    # df = db.to_df(f\"\"\"\n",
    "    #     SELECT\n",
    "    #         r.topic,\n",
    "    #         r.content_type,\n",
    "    #         r.exchange,\n",
    "    #         r.received,\n",
    "    #         r.body.sessionid,\n",
    "    #         TO_TIMESTAMP(r.body.\"on\", 'YYYY-MM-DD HH24:MM:SS') \n",
    "    #                                     AS computed_ts,\n",
    "    #         r.body.jornayaid,\n",
    "    #         r.body.score,\n",
    "    #         r.body.response.meta.model\n",
    "    #     FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "    #     WHERE\n",
    "    #         {start_date} <= computed_ts AND computed_ts <= {end_date}\n",
    "    #     LIMIT 100\n",
    "    # \"\"\")\n",
    "\n",
    "    # df = db.to_df(f\"\"\"\n",
    "    #     SELECT\n",
    "    #         r.topic,\n",
    "    #         r.content_type,\n",
    "    #         r.exchange,\n",
    "    #         r.received,\n",
    "    #         r.body.sessionid,\n",
    "    #         TO_DATE(r.body.\"on\", 'YYYY-MM-DD')      AS computed_dt,\n",
    "    #         r.body.jornayaid,\n",
    "    #         r.body.score,\n",
    "    #         r.body.response.meta.model\n",
    "    #     FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "    #     WHERE\n",
    "    #         ({start_date.year} < r.year OR ({start_date.year} <= r.year AND {start_date.month} <= r.month)) AND\n",
    "    #         (r.year < {end_date.year} OR (r.year <= {end_date.year} AND r.month <= {end_date.month})) AND\n",
    "    #         {start_date.date()} <= computed_dt AND computed_dt <= {end_date.date()}\n",
    "    #     LIMIT 100\n",
    "    # \"\"\")\n",
    "\n",
    "    df = db.to_df(f\"\"\"\n",
    "        SELECT\n",
    "            r.topic,\n",
    "            r.content_type,\n",
    "            r.exchange,\n",
    "            r.received,\n",
    "            r.body.sessionid,\n",
    "            TO_DATE(r.body.\"on\", 'YYYY-MM-DD')      AS computed_dt,\n",
    "            r.body.jornayaid,\n",
    "            r.body.score,\n",
    "            r.body.response.meta.model\n",
    "        FROM dl_landing.internal_kraken_leadscore_scored AS r\n",
    "        WHERE\n",
    "            /* Data partitioned on date - these filters greatly speed query */\n",
    "            (r.year > {start_date.year} OR \n",
    "                (r.year = {start_date.year} AND r.month >= {start_date.month})) \n",
    "            AND\n",
    "            (r.year < {end_date.year} OR \n",
    "                (r.year = {end_date.year} AND r.month <= {end_date.month})) \n",
    "        LIMIT 100\n",
    "    \"\"\")\n",
    "df_bkp = df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_filter = \"\" if product is None else \\\n",
    "    f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "traffic_filter = \"\" if traffic_source is None else \\\n",
    "    f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "session_revenue_sql = f\"\"\"\n",
    "SELECT\n",
    "    session_id,\n",
    "    sum(revenue) AS revenue\n",
    "FROM tron.session_revenue\n",
    "WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "    {product_filter}\n",
    "    {traffic_filter}\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "geoip_sql = f\"\"\"\n",
    "SELECT \n",
    "    l.*,\n",
    "    b.netowrk_index,\n",
    "    b.start_int,\n",
    "    b.end_int\n",
    "FROM \n",
    "    data_science.maxmind_ipv4_geo_blocks AS b\n",
    "    JOIN data_science.maxmind_geo_locations AS l\n",
    "        ON b.maxmind_id = l.maxmind_id\n",
    "\"\"\"\n",
    "sql_query = f\"\"\"\n",
    "with\n",
    "    rps as ({session_revenue_sql}),\n",
    "    ip_locs as ({geoip_sql}),\n",
    "    rps_tz_adj as (\n",
    "        SELECT\n",
    "            s.creation_date                                         AS utc_ts,\n",
    "            extract(\n",
    "                HOUR FROM\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                    - s.creation_date\n",
    "            )::INT                                                  AS utc_offset,\n",
    "            l.time_zone,\n",
    "            convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "            date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "            date_part(HOUR, user_ts) +\n",
    "            CASE \n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "            END                                                     AS hourofday,\n",
    "            l.subdivision_1_iso_code                                AS state,\n",
    "            l.metro_code                                            AS dma,\n",
    "            r.revenue\n",
    "        FROM \n",
    "            tracking.session_detail AS s\n",
    "            JOIN ip_locs as l\n",
    "                ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                AND l.country_iso_code = 'US'\n",
    "            INNER JOIN rps as r\n",
    "                ON s.session_id = r.session_id\n",
    "            LEFT JOIN \n",
    "        WHERE nullif(s.ip_address, '') IS NOT null\n",
    "            AND nullif(dma,'') IS NOT NULL \n",
    "            AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "    )\n",
    "SELECT\n",
    "    *\n",
    "FROM \n",
    "    rps_tz_adj\n",
    "LIMIT 100\n",
    ";\n",
    "\"\"\"\n",
    "with HealthcareDW() as db:\n",
    "    session_rps_df = df.to_df(sql_query)\n",
    "session_rps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_filter = \"\" if product is None else \\\n",
    "    f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "traffic_filter = \"\" if traffic_source is None else \\\n",
    "    f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "agg_rps_query = f\"\"\"\n",
    "with\n",
    "    rps as (\n",
    "        SELECT\n",
    "            session_id,\n",
    "            sum(revenue) AS revenue\n",
    "        FROM tron.session_revenue\n",
    "        WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "        GROUP BY 1\n",
    "    ),\n",
    "    ip_locs as (\n",
    "        SELECT \n",
    "            l.*,\n",
    "            b.netowrk_index,\n",
    "            b.start_int,\n",
    "            b.end_int\n",
    "        FROM \n",
    "            data_science.maxmind_ipv4_geo_blocks AS b\n",
    "            JOIN data_science.maxmind_geo_locations AS l\n",
    "                ON b.maxmind_id = l.maxmind_id\n",
    "    ),\n",
    "    rps_tz_adj as (\n",
    "        SELECT\n",
    "            s.creation_date                                         AS utc_ts,\n",
    "            extract(\n",
    "                HOUR FROM\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                    - s.creation_date\n",
    "            )::INT                                                  AS utc_offset,\n",
    "            l.time_zone,\n",
    "            convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "            date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "            date_part(HOUR, user_ts) +\n",
    "            CASE \n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "            END                                                     AS hourofday,\n",
    "            l.subdivision_1_iso_code                                AS state,\n",
    "            l.metro_code                                            AS dma,\n",
    "            r.revenue\n",
    "        FROM \n",
    "            tracking.session_detail AS s\n",
    "            JOIN ip_locs as l\n",
    "                ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                AND l.country_iso_code = 'US'\n",
    "            INNER JOIN rps as r\n",
    "                ON s.session_id = r.session_id\n",
    "        WHERE nullif(s.ip_address, '') IS NOT null\n",
    "            AND nullif(dma,'') IS NOT NULL \n",
    "            AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "            {product_filter}\n",
    "            {traffic_filter}\n",
    "    )\n",
    "SELECT\n",
    "    {','.join(agg_columns)},\n",
    "    COUNT(session_id)                                                       AS sessions,\n",
    "    SUM((revenue>0)::INT::FLOAT)                                            AS num_leads,\n",
    "    AVG((revenue>0)::INT::FLOAT)                                            AS lps_avg,\n",
    "    SUM(revenue) / CASE\n",
    "        WHEN num_leads = 0 THEN 1\n",
    "        ELSE num_leads\n",
    "    END                                                                     AS rpl_avg,\n",
    "    (SUM(revenue) / COUNT(DISTINCT session_id))::NUMERIC(8,4)               AS rps_,\n",
    "    AVG(revenue)                                                            AS rps_avg,\n",
    "    STDDEV(revenue)                                                         AS rps_std,\n",
    "    VARIANCE(revenue)                                                       AS rps_var\n",
    "FROM rps_tz_adj\n",
    "GROUP BY {','.join(agg_columns)}\n",
    "\"\"\"\n",
    "# print(agg_rps_query)\n",
    "# print(traffic_filter)\n",
    "from ds_utils.db.connectors import HealthcareDW\n",
    "with HealthcareDW() as db:\n",
    "    df = db.to_df(agg_rps_query)\n",
    "globals()[\"df\"] = df\n",
    "\n",
    "delt = df[\"rps_avg\"] - df['rps_']\n",
    "if not all(delt.abs() < 1e-3):\n",
    "    print(\"session uniqueness assummption not satisfied\")\n",
    "df = df \\\n",
    "    .sort_values(by=agg_columns, ascending=True) \\\n",
    "    .set_index(agg_columns)\n",
    "\n",
    "df['int_ix'] = range(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "import functools\n",
    "@functools.lru_cache()\n",
    "def agg_rps(start_date,end_date,product,traffic_source,agg_columns):\n",
    "    agg_columns = list(agg_columns)\n",
    "    product_filter = \"\" if product is None else \\\n",
    "        f\"AND UPPER(s.product) = UPPER('{product}')\"\n",
    "    traffic_filter = \"\" if traffic_source is None else \\\n",
    "        f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "    agg_rps_query = f\"\"\"\n",
    "    with\n",
    "        rps as (\n",
    "            SELECT\n",
    "                session_id,\n",
    "                sum(revenue) AS revenue\n",
    "            FROM tron.session_revenue\n",
    "            WHERE session_creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "                {product_filter}\n",
    "                {traffic_filter}\n",
    "            GROUP BY 1\n",
    "        ),\n",
    "        ip_locs as (\n",
    "            SELECT \n",
    "                l.*,\n",
    "                b.netowrk_index,\n",
    "                b.start_int,\n",
    "                b.end_int\n",
    "            FROM \n",
    "                data_science.maxmind_ipv4_geo_blocks AS b\n",
    "                JOIN data_science.maxmind_geo_locations AS l\n",
    "                    ON b.maxmind_id = l.maxmind_id\n",
    "        ),\n",
    "        rps_tz_adj as (\n",
    "            SELECT\n",
    "                s.*,\n",
    "                s.creation_date                                         AS utc_ts,\n",
    "                extract(\n",
    "                    HOUR FROM\n",
    "                    convert_timezone('UTC', l.time_zone, s.creation_date) \n",
    "                        - s.creation_date\n",
    "                )::INT                                                  AS utc_offset,\n",
    "                l.time_zone,\n",
    "                convert_timezone('UTC', l.time_zone, s.creation_date)   AS user_ts,\n",
    "                date_part(DOW, user_ts)::INT                            AS dayofweek,\n",
    "                date_part(HOUR, user_ts) +\n",
    "                CASE \n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 0 AND 14 THEN 0.0\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 15 AND 29 THEN 0.25\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 30 AND 44 THEN 0.5\n",
    "                    WHEN date_part(MINUTE, user_ts)::INT BETWEEN 45 AND 59 THEN 0.75\n",
    "                END                                                     AS hourofday,\n",
    "                l.subdivision_1_iso_code                                AS state,\n",
    "                l.metro_code                                            AS dma,\n",
    "                r.revenue\n",
    "            FROM \n",
    "                tracking.session_detail AS s\n",
    "                JOIN ip_locs as l\n",
    "                    ON ip_index(s.ip_address) = l.netowrk_index\n",
    "                    AND inet_aton(s.ip_address) BETWEEN l.start_int AND l.end_int\n",
    "                    AND l.country_iso_code = 'US'\n",
    "                INNER JOIN rps as r\n",
    "                    ON s.session_id = r.session_id\n",
    "            WHERE nullif(s.ip_address, '') IS NOT null\n",
    "                AND nullif(dma,'') IS NOT NULL \n",
    "                AND s.creation_date::DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "                {product_filter}\n",
    "                {traffic_filter}\n",
    "        )\n",
    "    SELECT\n",
    "        {','.join(agg_columns)},\n",
    "        COUNT(session_id)                                                       AS sessions,\n",
    "        SUM((revenue>0)::INT::FLOAT)                                            AS num_leads,\n",
    "        AVG((revenue>0)::INT::FLOAT)                                            AS lps_avg,\n",
    "        SUM(revenue) / CASE\n",
    "            WHEN num_leads = 0 THEN 1\n",
    "            ELSE num_leads\n",
    "        END                                                                     AS rpl_avg,\n",
    "        (SUM(revenue) / COUNT(DISTINCT session_id))::NUMERIC(8,4)               AS rps_,\n",
    "        AVG(revenue)                                                            AS rps_avg,\n",
    "        STDDEV(revenue)                                                         AS rps_std,\n",
    "        VARIANCE(revenue)                                                       AS rps_var\n",
    "    FROM rps_tz_adj\n",
    "    GROUP BY {','.join(agg_columns)}\n",
    "    \"\"\"\n",
    "    # print(agg_rps_query)\n",
    "    # print(traffic_filter)\n",
    "    from ds_utils.db.connectors import HealthcareDW\n",
    "    with HealthcareDW() as db:\n",
    "        df = db.to_df(agg_rps_query)\n",
    "    globals()[\"df\"] = df\n",
    "\n",
    "    delt = df[\"rps_avg\"] - df['rps_']\n",
    "    if not all(delt.abs() < 1e-3):\n",
    "        print(\"session uniqueness assummption not satisfied\")\n",
    "    df = df \\\n",
    "        .sort_values(by=agg_columns, ascending=True) \\\n",
    "        .set_index(agg_columns)\n",
    "\n",
    "    df['int_ix'] = range(len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "campaign_split_fields = dict(\n",
    "    # traffic_source = [\"traffic_source\"],\n",
    "    browser = [\"browser\"],\n",
    "    operating_system = [\"operating_system\"],\n",
    "    device = [\"device\"],\n",
    "    # channel = [\"channel\"],\n",
    "    # domain = [\"domain\"],\n",
    "    product = [\"product\"],\n",
    "    # keyword = [\"keyword\"],\n",
    "    # campaign_id = [\"campaign_id\"],\n",
    "    # landing_page = [\"landing_page\"],\n",
    "    TOD = [\"dayofweek\",\"hourofday\"],\n",
    "    dma = [\"dma\"],\n",
    "    state =[\"state\",],\n",
    "    location = [\"state\",\"dma\"],\n",
    "    \n",
    "    dma_os=[\"dma\", \"operating_system\"],\n",
    "    dma_device=[\"dma\", \"device\", ],\n",
    "    dma_os_device=[\"dma\", \"operating_system\", \"device\"],\n",
    "\n",
    "    state_os=[\"state\", \"operating_system\"],\n",
    "    state_device=[\"state\", \"device\", ],\n",
    "    state_os_device=[\"state\", \"operating_system\", \"device\"],\n",
    "\n",
    "    location_os = [\"state\", \"dma\", \"operating_system\"],\n",
    "    location_device=[\"state\", \"dma\", \"device\", ],\n",
    "    location_os_device = [\"state\", \"dma\", \"operating_system\",\"device\"],\n",
    ")\n",
    "\n",
    "taboola_val_map = {\n",
    "    \"device\": {\n",
    "        'DESKTOP': 'DESK',\n",
    "        'MOBILE': 'PHON',\n",
    "        'TABLET': 'TBLT',\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        'Linux armv7l': \"Linux\",\n",
    "        'Linux armv8l': \"Linux\",\n",
    "        'Linux x86_64': \"Linux\",\n",
    "        'MacIntel': 'Mac OS X',\n",
    "        'Win32': \"Windows\",\n",
    "        'iPad': \"iPadOS\",\n",
    "        'iPhone': \"iOS\",\n",
    "        '': None,\n",
    "        'ARM': None,\n",
    "        'Android': 'Android',\n",
    "        'Linux aarch64': \"Linux\",\n",
    "        'Win64': \"Windows\",\n",
    "        'Linux armv7': \"Linux\",\n",
    "        'Linux i686': \"Linux\",\n",
    "        'Windows': \"Windows\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def translate_taboola_vals(df):\n",
    "    index_cols = df.index.names\n",
    "    df = df.reset_index()\n",
    "    for c in df.columns:\n",
    "        if c in taboola_val_map:\n",
    "            df[c] = df[c].apply(taboola_val_map[c].__getitem__)\n",
    "    df_bkp = df\n",
    "    df = df \\\n",
    "        .groupby(index_cols) \\\n",
    "        .agg({\n",
    "            \"sessions\": sum,\n",
    "            \"num_leads\": sum,\n",
    "            \"lps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rpl_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "        })\n",
    "    df[\"int_ix\"] = range(len(df))\n",
    "    df_bkp_wavg = wavg(df_bkp[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                        df_bkp[\"sessions\"].values.reshape(-1, 1))\n",
    "    df_wavg = wavg(df[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                    df[\"sessions\"].values.reshape(-1, 1))\n",
    "    assert all((df_bkp_wavg - df_wavg).abs() < 1e-2), (df_bkp_wavg,df_wavg)\n",
    "    return df\n",
    "\n",
    "import pprint\n",
    "from IPython.display import display as ipydisp    \n",
    "import pandas as pd\n",
    "from models.utils import wavg\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def wvar(V,W):\n",
    "    mu = wavg(V,W)\n",
    "    var = wavg((V - mu)**2,W)\n",
    "    return var\n",
    "def wstd(V,W):\n",
    "    return wvar(V,W)**0.5\n",
    "def get_wavg_by(df, col):\n",
    "    def wavg_by(V):\n",
    "        return wavg(V, W=df.loc[V.index, col])\n",
    "    return wavg_by\n",
    "\n",
    "def get_wthresh(W,p):\n",
    "    W = rps_df[\"sessions\"].sort_values(ascending=False)\n",
    "    Wsum = W.sum()\n",
    "    cumsum = 0\n",
    "    for wthresh in W:\n",
    "        if cumsum > Wsum * p:\n",
    "            break\n",
    "        cumsum += wthresh\n",
    "    return wthresh\n",
    "\n",
    "\"\"\"\n",
    "TODO: 2021-05-20\n",
    "  - test fitting on general traffic sources?\n",
    "  - test clustering on multiple days of rps data - \n",
    "      or rolling rps data or something\n",
    "  - try out more granular aggs\n",
    "  - is there some kind of metric which measures total variance\n",
    "      and computes how much of that variance is captured by a split?\n",
    "\n",
    "Trevor: 2021-05-21\n",
    "TODO:\n",
    "- figure out how to minimize campaign # when writing back to taboola\n",
    "    - want to make sure campaigns have sufficient traffic\n",
    "- kw=(location,os,device)\n",
    "- calc 30-day rps/kw\n",
    "- can create distribution over the rps(kw) distribution\n",
    "- 100 campaigns - 1 per percentile\n",
    "\n",
    "TODO: 2021-05-24\n",
    "- look into clustering each individual split variable\n",
    "\"\"\"\n",
    "\n",
    "# split2aggrps = {}\n",
    "# for split,split_cols in campaign_split_fields.items():\n",
    "#     print(split,split_cols)\n",
    "#     rps_df = agg_rps(NOW-90*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=split_cols)\n",
    "#     rps_df = translate_taboola_vals(rps_df)\n",
    "#     rps_df[\"split_on\"] = split\n",
    "#     split2aggrps[split] = rps_df\n",
    "#     print(split,rps_df.shape)\n",
    "\n",
    "\n",
    "def agg_rps(start_date, end_date, product, traffic_source, agg_columns):\n",
    "\n",
    "\n",
    "def agg_rps_taboola(start_date, end_date, product, traffic_source, agg_columns):\n",
    "    rps_df = agg_rps(start_date,end_date,None,traffic_source=traffic_source,agg_columns=agg_columns)\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "\n",
    "split2fitaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-90*DAY,NOW-30*DAY,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2fitaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n",
    "\n",
    "split2evalaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-30*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2evalaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.cluster\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "def cluster_split(split):\n",
    "    rps_df_fit = split2fitaggrps[split]\n",
    "    rps_df_eval = split2evalaggrps[split]\n",
    "    split_size = rps_df_fit.__len__()\n",
    "    MINCLUST = 8\n",
    "    if split_size <= MINCLUST:\n",
    "        rps_clust_df = rps_df_eval\n",
    "        rps_df_fit[\"clust\"] = 0\n",
    "        rps_df_eval[\"clust\"] = 0\n",
    "    else:\n",
    "        # nclust = MINCLUST\n",
    "        nclust = max(MINCLUST, int(np.log(split_size)))\n",
    "        # nclust = max(MINCLUST,int(split_size ** 0.5))\n",
    "        # print(\"nclust\", nclust, split_size, np.log(split_size))\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=nclust)\n",
    "        rps_df_fit[\"clust\"] = clusterer \\\n",
    "            .fit_predict(\n",
    "                rps_df_fit[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_fit[\"sessions\"])\n",
    "        rps_df_eval[\"clust\"] = clusterer \\\n",
    "            .predict(\n",
    "                rps_df_eval[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_eval[\"sessions\"])\n",
    "        rps_clust_df = rps_df_eval \\\n",
    "            .groupby(\"clust\") \\\n",
    "            .agg({\"rps_avg\": get_wavg_by(rps_df_eval, \"sessions\"), \"sessions\": sum})\n",
    "    assert rps_clust_df[\"rps_avg\"].max() <= rps_df_eval[\"rps_avg\"].max()\n",
    "    rps_wavg = wavg(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"])\n",
    "    rps_clust_wavg = wavg(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"])\n",
    "    assert abs(rps_wavg - rps_clust_wavg) < 1e-4, (rps_wavg, rps_clust_wavg)\n",
    "    return rps_df_fit,rps_df_eval,rps_clust_df\n",
    "\n",
    "def get_split_factor(rps_df):\n",
    "    orig_index = rps_df.index.names\n",
    "    split_attr2unique_vals = {index_col: rps_df.index.unique(\n",
    "        index_col) for index_col in orig_index}\n",
    "    _,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "    nclusts = rps_df[\"clust\"].unique().__len__()\n",
    "    split_factor = nclusts * np.prod([1] + [split_attr2unique_vals[c].__len__() for c in new_index_order[:-1]])\n",
    "    return split_factor\n",
    "\n",
    "perfD = []\n",
    "for split in campaign_split_fields.keys():\n",
    "    rps_df_fit,rps_df_eval,rps_clust_df = cluster_split(split)\n",
    "    perfd = {\n",
    "        \"split\": split,\n",
    "        \"fit_shape\": rps_df_fit.shape,\n",
    "        \"clust_shape\": rps_clust_df.shape,\n",
    "        # wavg(rps_df[\"rps_avg\"],rps_df[\"sessions\"]),\n",
    "        \"split_variance\": wstd(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"]),\n",
    "        \"cluster_variance\": wstd(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"]),\n",
    "        # wstd(rps_df[\"rps_avg\"],rps_df[\"sessions\"])\n",
    "        \"split_factor\": rps_df_fit.__len__(),\n",
    "        \"clustered_split_factor\": get_split_factor(rps_df_fit),\n",
    "    }\n",
    "    perfD.append(perfd)\n",
    "    pprint.pprint(perfd)\n",
    "    ipydisp(rps_clust_df)\n",
    "\n",
    "perfdf = pd.DataFrame(perfD)\n",
    "ipydisp(perfdf)\n",
    "#%%\n",
    "# BEST_SPLIT = \"location_os_device\"\n",
    "BEST_SPLIT = \"dma_os_device\"\n",
    "# BEST_SPLIT = \"dma_os\"\n",
    "\n",
    "rps_df_fit, rps_df_eval, rps_clust_df = cluster_split(BEST_SPLIT)\n",
    "rps_df = rps_df_fit\n",
    "get_split_factor(rps_df)\n",
    "orig_index = rps_df.index.names\n",
    "split_attr2unique_vals = {index_col: rps_df.index.unique(index_col) for index_col in orig_index}\n",
    "_,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "rps_df = rps_df .reset_index()\n",
    "campaign_df = rps_df \\\n",
    "    .groupby([*new_index_order[:-1], \"clust\"]) \\\n",
    "    .agg({\n",
    "        \"sessions\": sum,\n",
    "        \"rps_avg\": get_wavg_by(rps_df,\"sessions\"),\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "assert campaign_df[\"sessions\"].sum() == rps_df_fit[\"sessions\"].sum()\n",
    "camp_rps_wavg = wavg(campaign_df[\"rps_avg\"],campaign_df[\"sessions\"])\n",
    "fit_rps_wavg = wavg(rps_df_fit[\"rps_avg\"], rps_df_fit[\"sessions\"])\n",
    "assert abs(camp_rps_wavg - fit_rps_wavg) < 1e-5\n",
    "\n",
    "excl_campaign_df = campaign_df.groupby([*new_index_order[:-1]]) \\\n",
    "    .agg({\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "def flatten(M):\n",
    "    return tuple(el for r in M for el in r)\n",
    "excl_campaign_df[new_index_order[-1]] = excl_campaign_df[new_index_order[-1]] .apply(flatten)\n",
    "\n",
    "camps = []\n",
    "for idx,r in campaign_df.iterrows():\n",
    "    camp = {\n",
    "        \"sessions_60d\": r[\"sessions\"], \n",
    "        \"rps_avg_60d\": r[\"rps_avg\"]\n",
    "    }\n",
    "    for field,val in zip(new_index_order[:-1],idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"includes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "for idx, r in excl_campaign_df.iterrows():\n",
    "    camp = {}\n",
    "    for field, val in zip(new_index_order[:-1], idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"excludes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "camp_df = pd.DataFrame(camps)\n",
    "camp_df.to_csv(\"campaign_dump.csv\")\n",
    "#%%\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn.feature_selection\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "\n",
    "requires_campaign_split = [\n",
    "    # \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    # \"domain\",\n",
    "    \"product\",\n",
    "    # \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "\n",
    "for split in requires_campaign_split:\n",
    "    rps_df = split2aggrps[split] .reset_index()\n",
    "    Xs = rps_df \\\n",
    "        .apply(lambda r: [r[\"int_ix\"]]*int(r[\"sessions\"]), axis=1)\n",
    "    X = np.concatenate(Xs.values).reshape(-1,1)\n",
    "    ys = rps_df \\\n",
    "        .apply(lambda r: [r[\"rps_avg\"]]*int(r[\"sessions\"]),axis=1)\n",
    "    y = np.concatenate(ys.values)\n",
    "    # print(y.min(),np.quantile(y, 0.5),y.max())\n",
    "    y = y > y.mean()\n",
    "    # y = np.concatenate(ys.values).reshape(-1,1)\n",
    "    # y = sklearn.preprocessing.KBinsDiscretizer(n_bins=2,encode=\"ordinal\") \\\n",
    "    #     .fit_transform(y).reshape(-1)\n",
    "    mi = sklearn.feature_selection.mutual_info_regression(X,y,discrete_features=True)\n",
    "    print(split,mi,y.mean(),rps_df.shape)\n",
    "#%%\n",
    "y\n",
    "#%%\n",
    "sklearn.metrics.mutual_info_score\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "requires_campaign_split = [\n",
    "    \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    # \"domain\",\n",
    "    \"product\",\n",
    "    # \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "combined_rps_df = pd.concat([df.reset_index() for df in split2aggrps.values()])\n",
    "splitI = combined_rps_df[\"split_on\"].isin(requires_campaign_split)\n",
    "density = scipy.stats.gaussian_kde(\n",
    "    dataset=combined_rps_df.loc[splitI,\"rps_avg\"],\n",
    "    weights=combined_rps_df.loc[splitI, \"sessions\"],\n",
    "    # bw_method=\"scott\",\n",
    "    # bw_method=\"silverman\",\n",
    "    # bw_method=0.1,\n",
    ")\n",
    "xs = np.linspace(0,2,100)\n",
    "plt.plot(xs,density(xs))\n",
    "#%%\n",
    "for split in requires_campaign_split:\n",
    "    splitI = combined_rps_df[\"split_on\"] == split\n",
    "    density = scipy.stats.gaussian_kde(\n",
    "        dataset=combined_rps_df.loc[splitI, \"rps_avg\"],\n",
    "        weights=combined_rps_df.loc[splitI, \"sessions\"])\n",
    "    xs = np.linspace(0, 2, 100)\n",
    "    plt.plot(xs, density(xs))\n",
    "    plt.title(split)\n",
    "    plt.show()\n",
    "#%%\n",
    "combined_rps_df.loc[splitI, [\"rps_avg\",\"sessions\"]].apply(\n",
    "        lambda r: pd.Series([r[\"rps_avg\"]]*int(r[\"sessions\"])),axis=1) \\\n",
    "    .unstack()\n",
    "#%%\n",
    "agg_rps = split2aggrps[\"TOD\"]\n",
    "Xy = agg_rps[[\"sessions\",\"rps\"]].reset_index()\n",
    "Xy[\"i\"] = range(len(Xy))\n",
    "Xy\n",
    "#%%\n",
    "import sklearn.feature_selection\n",
    "sklearn.feature_selection.mutual_info_regression(\n",
    "    []\n",
    ")\n",
    "\n",
    "#%%\n",
    "\"\"\"\n",
    "- \n",
    "- overall goal:\n",
    "    - specific ROI targetting w/ minimal campaigns\n",
    "- what this means for accnt structure\n",
    "    - for many variables we must split campaigns to target ROI\n",
    "    - want to capture greatest amt of rps variation w/ \n",
    "        fewest # of campaigns\n",
    "- 2 ways of approaching this\n",
    "    1. minimize rps variation w/in a campaign\n",
    "        - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "        - tried this out a little - but the computation was intensive which made it slow going\n",
    "    2. maximize rps variation outside campaigns\n",
    "    - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "    - mostly focused on this approach\n",
    "\n",
    "- approaches I tried\n",
    "1. rank splits by their correlation/covaraince/MI w/ `agg_rps`\n",
    "    - tried ANOVA,pt.biserial,MI - had most success w/ MI\n",
    "    - was dificult to compute b/c couldnt find methods that accepted sample weight\n",
    "    - MI approach was promising but didnt go down that route\n",
    "2. rank splits by intra-split agg_rps variance\n",
    "    - had most success w/ this\n",
    "    - tested out an approahc where I cluster the split on rps - had good results\n",
    "    - think this is the mtd to use going forward\n",
    "    TODO:\n",
    "    - test fitting on general traffic sources?\n",
    "    - test clustering on multiple days of rps data - \n",
    "        or rolling rps data or something\n",
    "    - try out more granular aggs\n",
    "    - is there some kind of metric which measures total variance\n",
    "        and computes how much of that variance is captured by a split?\n",
    "\n",
    "3. fit decision tree on rps data \n",
    "    - MSE criterion is apparently the same as minimizng inter split variance\n",
    "    - computationally intensive\n",
    "    - not 100% clear how to go from tree to campaign structure\n",
    "\n",
    "\n",
    "- can do this by\n",
    "    1. choosing what vars or tuples of vars to split campaigns on\n",
    "    2. grouping similar buckets w/in those splits\n",
    "- want to measure correlation/dependence of categorical split vars w/ cont rps\n",
    "    - cat,cat metrics w/ binned rps\n",
    "        chi\n",
    "    - cont,cont metrcs w/ 1 hot encoded split vars\n",
    "    - cat,cont metrics\n",
    "- correlation metrics\n",
    "    - MI:\n",
    "        - sklearn.feature_selection.mi_regression\n",
    "        - would need AFAICT to rresample input arrays\n",
    "    - ANOVA:\n",
    "        - scipy.stats.f_oneway\n",
    "    - pt biserial\n",
    "        - needs binary vars tho\n",
    "        - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html\n",
    "        - https://towardsdatascience.com/point-biserial-correlation-with-python-f7cd591bd3b1\n",
    "- want most variation of rps w/ fewest camapaigns\n",
    "- dont want to group similar buckets together - think its too complicated\n",
    "\n",
    "\"\"\"\n",
    "#%%\n",
    "requires_campaign_split = [\n",
    "    \"browser\",\n",
    "    \"operating_system\",\n",
    "    \"device\",\n",
    "    # \"channel\", # not actually sure what this is\n",
    "    \"domain\",\n",
    "    \"product\",\n",
    "    \"landing_page\",\n",
    "    \"location\",\n",
    "]\n",
    "#%%\n",
    "\n",
    "traffic_source = TABOOLA\n",
    "with HealthcareDW() as db:\n",
    "    traffic_filter = \"\" if traffic_source is None else \\\n",
    "        f\"AND UPPER(traffic_source) = UPPER('{traffic_source}')\"\n",
    "    sql = f\"\"\"\n",
    "    select\n",
    "        traffic_source,domain,count(*)\n",
    "    from tracking.session_detail\n",
    "    where True \n",
    "    {traffic_filter}\n",
    "    group by traffic_source,domain;\n",
    "    \"\"\"\n",
    "    df = db.to_df(sql).sort_values(\"count\")\n",
    "df\n",
    "#%%\n",
    "\n",
    "# %%\n",
    "with HealthcareDW() as db:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
