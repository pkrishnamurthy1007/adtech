{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- overall goal:\n",
    "    - specific ROI targetting w/ minimal campaigns\n",
    "- what this means for accnt structure\n",
    "    - for many variables we must split campaigns to target ROI\n",
    "    - want to capture greatest amt of rps variation w/ \n",
    "        fewest # of campaigns\n",
    "- 2 ways of approaching this\n",
    "    1. minimize rps variation w/in a campaign\n",
    "        - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "        - tried this out a little - but the computation was intensive which made it slow going\n",
    "    2. maximize rps variation outside campaigns\n",
    "    - i.e. after campaign split want to minimize rps variance w/in campaigns\n",
    "        => i actually think this is eq to decision tree regression w/ split criterion MSE\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "    - mostly focused on this approach\n",
    "\n",
    "- approaches I tried\n",
    "1. rank splits by their correlation/covaraince/MI w/ `agg_rps`\n",
    "    - tried ANOVA,pt.biserial,MI - had most success w/ MI\n",
    "    - was dificult to compute b/c couldnt find methods that accepted sample weight\n",
    "    - MI approach was promising but didnt go down that route\n",
    "2. rank splits by intra-split agg_rps variance\n",
    "    - had most success w/ this\n",
    "    - tested out an approahc where I cluster the split on rps - had good results\n",
    "    - think this is the mtd to use going forward\n",
    "    TODO:\n",
    "    - test fitting on general traffic sources?\n",
    "    - test clustering on multiple days of rps data - \n",
    "        or rolling rps data or something\n",
    "    - try out more granular aggs\n",
    "    - is there some kind of metric which measures total variance\n",
    "        and computes how much of that variance is captured by a split?\n",
    "\n",
    "3. fit decision tree on rps data \n",
    "    - MSE criterion is apparently the same as minimizng inter split variance\n",
    "    - computationally intensive\n",
    "    - not 100% clear how to go from tree to campaign structure\n",
    "\n",
    "\n",
    "- can do this by\n",
    "    1. choosing what vars or tuples of vars to split campaigns on\n",
    "    2. grouping similar buckets w/in those splits\n",
    "- want to measure correlation/dependence of categorical split vars w/ cont rps\n",
    "    - cat,cat metrics w/ binned rps\n",
    "        chi\n",
    "    - cont,cont metrcs w/ 1 hot encoded split vars\n",
    "    - cat,cont metrics\n",
    "- correlation metrics\n",
    "    - MI:\n",
    "        - sklearn.feature_selection.mi_regression\n",
    "        - would need AFAICT to rresample input arrays\n",
    "    - ANOVA:\n",
    "        - scipy.stats.f_oneway\n",
    "    - pt biserial\n",
    "        - needs binary vars tho\n",
    "        - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html\n",
    "        - https://towardsdatascience.com/point-biserial-correlation-with-python-f7cd591bd3b1\n",
    "- want most variation of rps w/ fewest camapaigns\n",
    "- dont want to group similar buckets together - think its too complicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.env import load_env_from_aws\n",
    "load_env_from_aws()\n",
    "\n",
    "import datetime\n",
    "from notebooks.aduriseti_shared.utils import *\n",
    "\n",
    "TABOOLA = \"TABOOLA\"\n",
    "MEDIA_ALPHA = \"MEDIAALPHA\"\n",
    "BING = \"BING\"\n",
    "U65 = \"HEALTH\"\n",
    "O65 = 'MEDICARE'\n",
    "\n",
    "NOW = datetime.datetime.now()\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "campaign_split_fields = dict(\n",
    "    # traffic_source = [\"traffic_source\"],\n",
    "    browser = [\"browser\"],\n",
    "    operating_system = [\"operating_system\"],\n",
    "    device = [\"device\"],\n",
    "    # channel = [\"channel\"],\n",
    "    # domain = [\"domain\"],\n",
    "    product = [\"product\"],\n",
    "    # keyword = [\"keyword\"],\n",
    "    # campaign_id = [\"campaign_id\"],\n",
    "    # landing_page = [\"landing_page\"],\n",
    "    TOD = [\"dayofweek\",\"hourofday\"],\n",
    "    dma = [\"dma\"],\n",
    "    state =[\"state\",],\n",
    "    location = [\"state\",\"dma\"],\n",
    "    \n",
    "    dma_os=[\"dma\", \"operating_system\"],\n",
    "    dma_device=[\"dma\", \"device\", ],\n",
    "    dma_os_device=[\"dma\", \"operating_system\", \"device\"],\n",
    "\n",
    "    state_os=[\"state\", \"operating_system\"],\n",
    "    state_device=[\"state\", \"device\", ],\n",
    "    state_os_device=[\"state\", \"operating_system\", \"device\"],\n",
    "\n",
    "    location_os = [\"state\", \"dma\", \"operating_system\"],\n",
    "    location_device=[\"state\", \"dma\", \"device\", ],\n",
    "    location_os_device = [\"state\", \"dma\", \"operating_system\",\"device\"],\n",
    ")\n",
    "\n",
    "taboola_val_map = {\n",
    "    \"device\": {\n",
    "        'DESKTOP': 'DESK',\n",
    "        'MOBILE': 'PHON',\n",
    "        'TABLET': 'TBLT',\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        'Linux armv7l': \"Linux\",\n",
    "        'Linux armv8l': \"Linux\",\n",
    "        'Linux x86_64': \"Linux\",\n",
    "        'MacIntel': 'Mac OS X',\n",
    "        'Win32': \"Windows\",\n",
    "        'iPad': \"iPadOS\",\n",
    "        'iPhone': \"iOS\",\n",
    "        '': None,\n",
    "        'ARM': None,\n",
    "        'Android': 'Android',\n",
    "        'Linux aarch64': \"Linux\",\n",
    "        'Win64': \"Windows\",\n",
    "        'Linux armv7': \"Linux\",\n",
    "        'Linux i686': \"Linux\",\n",
    "        'Windows': \"Windows\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def translate_taboola_vals(df):\n",
    "    index_cols = df.index.names\n",
    "    df = df.reset_index()\n",
    "    for c in df.columns:\n",
    "        if c in taboola_val_map:\n",
    "            df[c] = df[c].apply(taboola_val_map[c].__getitem__)\n",
    "    df_bkp = df\n",
    "    df = df \\\n",
    "        .groupby(index_cols) \\\n",
    "        .agg({\n",
    "            \"sessions\": sum,\n",
    "            \"num_leads\": sum,\n",
    "            \"lps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rpl_avg\": get_wavg_by(df,\"sessions\"),\n",
    "            \"rps_avg\": get_wavg_by(df,\"sessions\"),\n",
    "        })\n",
    "    df[\"int_ix\"] = range(len(df))\n",
    "    df_bkp_wavg = wavg(df_bkp[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                        df_bkp[\"sessions\"].values.reshape(-1, 1))\n",
    "    df_wavg = wavg(df[[\"lps_avg\",\"rpl_avg\",\"rps_avg\"]],\n",
    "                    df[\"sessions\"].values.reshape(-1, 1))\n",
    "    assert all((df_bkp_wavg - df_wavg).abs() < 1e-2), (df_bkp_wavg,df_wavg)\n",
    "    return df\n",
    "\n",
    "import pprint\n",
    "from IPython.display import display as ipydisp    \n",
    "import pandas as pd\n",
    "from models.utils import wavg\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def wvar(V,W):\n",
    "    mu = wavg(V,W)\n",
    "    var = wavg((V - mu)**2,W)\n",
    "    return var\n",
    "def wstd(V,W):\n",
    "    return wvar(V,W)**0.5\n",
    "def get_wavg_by(df, col):\n",
    "    def wavg_by(V):\n",
    "        return wavg(V, W=df.loc[V.index, col])\n",
    "    return wavg_by\n",
    "\n",
    "def get_wthresh(W,p):\n",
    "    W = rps_df[\"sessions\"].sort_values(ascending=False)\n",
    "    Wsum = W.sum()\n",
    "    cumsum = 0\n",
    "    for wthresh in W:\n",
    "        if cumsum > Wsum * p:\n",
    "            break\n",
    "        cumsum += wthresh\n",
    "    return wthresh\n",
    "\n",
    "\"\"\"\n",
    "TODO: 2021-05-20\n",
    "  - test fitting on general traffic sources?\n",
    "  - test clustering on multiple days of rps data - \n",
    "      or rolling rps data or something\n",
    "  - try out more granular aggs\n",
    "  - is there some kind of metric which measures total variance\n",
    "      and computes how much of that variance is captured by a split?\n",
    "\n",
    "Trevor: 2021-05-21\n",
    "TODO:\n",
    "- figure out how to minimize campaign # when writing back to taboola\n",
    "    - want to make sure campaigns have sufficient traffic\n",
    "- kw=(location,os,device)\n",
    "- calc 30-day rps/kw\n",
    "- can create distribution over the rps(kw) distribution\n",
    "- 100 campaigns - 1 per percentile\n",
    "\n",
    "TODO: 2021-05-24\n",
    "- look into clustering each individual split variable\n",
    "\"\"\"\n",
    "\n",
    "# split2aggrps = {}\n",
    "# for split,split_cols in campaign_split_fields.items():\n",
    "#     print(split,split_cols)\n",
    "#     rps_df = agg_rps(NOW-90*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=split_cols)\n",
    "#     rps_df = translate_taboola_vals(rps_df)\n",
    "#     rps_df[\"split_on\"] = split\n",
    "#     split2aggrps[split] = rps_df\n",
    "#     print(split,rps_df.shape)\n",
    "\n",
    "\n",
    "split2fitaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-90*DAY,NOW-30*DAY,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2fitaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)\n",
    "\n",
    "split2evalaggrps = {}\n",
    "for split,split_cols in campaign_split_fields.items():\n",
    "    print(split,split_cols)\n",
    "    rps_df = agg_rps(NOW-30*DAY,NOW,None,traffic_source=TABOOLA,agg_columns=tuple(split_cols))\n",
    "    rps_df = translate_taboola_vals(rps_df)\n",
    "    rps_df[\"split_on\"] = split\n",
    "    split2evalaggrps[split] = rps_df\n",
    "\n",
    "    print(split,rps_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from matplotlib import pyplot as plt\n",
    "def cluster_split(split):\n",
    "    rps_df_fit = split2fitaggrps[split]\n",
    "    rps_df_eval = split2evalaggrps[split]\n",
    "    split_size = rps_df_fit.__len__()\n",
    "    MINCLUST = 8\n",
    "    if split_size <= MINCLUST:\n",
    "        rps_clust_df = rps_df_eval\n",
    "        rps_df_fit[\"clust\"] = 0\n",
    "        rps_df_eval[\"clust\"] = 0\n",
    "    else:\n",
    "        # nclust = MINCLUST\n",
    "        nclust = max(MINCLUST, int(np.log(split_size)))\n",
    "        # nclust = max(MINCLUST,int(split_size ** 0.5))\n",
    "        # print(\"nclust\", nclust, split_size, np.log(split_size))\n",
    "        clusterer = sklearn.cluster.KMeans(n_clusters=nclust)\n",
    "        rps_df_fit[\"clust\"] = clusterer \\\n",
    "            .fit_predict(\n",
    "                rps_df_fit[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_fit[\"sessions\"])\n",
    "        rps_df_eval[\"clust\"] = clusterer \\\n",
    "            .predict(\n",
    "                rps_df_eval[\"rps_avg\"].values.reshape(-1, 1),\n",
    "                sample_weight=rps_df_eval[\"sessions\"])\n",
    "        rps_clust_df = rps_df_eval \\\n",
    "            .groupby(\"clust\") \\\n",
    "            .agg({\"rps_avg\": get_wavg_by(rps_df_eval, \"sessions\"), \"sessions\": sum})\n",
    "    assert rps_clust_df[\"rps_avg\"].max() <= rps_df_eval[\"rps_avg\"].max()\n",
    "    rps_wavg = wavg(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"])\n",
    "    rps_clust_wavg = wavg(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"])\n",
    "    assert abs(rps_wavg - rps_clust_wavg) < 1e-4, (rps_wavg, rps_clust_wavg)\n",
    "    return rps_df_fit,rps_df_eval,rps_clust_df\n",
    "\n",
    "def get_split_factor(rps_df):\n",
    "    orig_index = rps_df.index.names\n",
    "    split_attr2unique_vals = {index_col: rps_df.index.unique(\n",
    "        index_col) for index_col in orig_index}\n",
    "    _,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "    nclusts = rps_df[\"clust\"].unique().__len__()\n",
    "    split_factor = nclusts * np.prod([1] + [split_attr2unique_vals[c].__len__() for c in new_index_order[:-1]])\n",
    "    return split_factor\n",
    "\n",
    "perfD = []\n",
    "for split in campaign_split_fields.keys():\n",
    "    rps_df_fit,rps_df_eval,rps_clust_df = cluster_split(split)\n",
    "    perfd = {\n",
    "        \"split\": split,\n",
    "        \"fit_shape\": rps_df_fit.shape,\n",
    "        \"clust_shape\": rps_clust_df.shape,\n",
    "        # wavg(rps_df[\"rps_avg\"],rps_df[\"sessions\"]),\n",
    "        \"split_variance\": wstd(rps_df_eval[\"rps_avg\"], rps_df_eval[\"sessions\"]),\n",
    "        \"cluster_variance\": wstd(rps_clust_df[\"rps_avg\"], rps_clust_df[\"sessions\"]),\n",
    "        # wstd(rps_df[\"rps_avg\"],rps_df[\"sessions\"])\n",
    "        \"split_factor\": rps_df_fit.__len__(),\n",
    "        \"clustered_split_factor\": get_split_factor(rps_df_fit),\n",
    "    }\n",
    "    perfD.append(perfd)\n",
    "    pprint.pprint(perfd)\n",
    "    ipydisp(rps_clust_df)\n",
    "\n",
    "perfdf = pd.DataFrame(perfD)\n",
    "ipydisp(perfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST_SPLIT = \"location_os_device\"\n",
    "BEST_SPLIT = \"dma_os_device\"\n",
    "# BEST_SPLIT = \"dma_os\"\n",
    "\n",
    "rps_df_fit, rps_df_eval, rps_clust_df = cluster_split(BEST_SPLIT)\n",
    "rps_df = rps_df_fit\n",
    "get_split_factor(rps_df)\n",
    "orig_index = rps_df.index.names\n",
    "split_attr2unique_vals = {index_col: rps_df.index.unique(index_col) for index_col in orig_index}\n",
    "_,new_index_order = zip(*sorted((V.__len__(),c) for c,V in split_attr2unique_vals.items()))\n",
    "rps_df = rps_df .reset_index()\n",
    "campaign_df = rps_df \\\n",
    "    .groupby([*new_index_order[:-1], \"clust\"]) \\\n",
    "    .agg({\n",
    "        \"sessions\": sum,\n",
    "        \"rps_avg\": get_wavg_by(rps_df,\"sessions\"),\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "assert campaign_df[\"sessions\"].sum() == rps_df_fit[\"sessions\"].sum()\n",
    "camp_rps_wavg = wavg(campaign_df[\"rps_avg\"],campaign_df[\"sessions\"])\n",
    "fit_rps_wavg = wavg(rps_df_fit[\"rps_avg\"], rps_df_fit[\"sessions\"])\n",
    "assert abs(camp_rps_wavg - fit_rps_wavg) < 1e-5\n",
    "\n",
    "excl_campaign_df = campaign_df.groupby([*new_index_order[:-1]]) \\\n",
    "    .agg({\n",
    "        new_index_order[-1]: tuple\n",
    "    })\n",
    "def flatten(M):\n",
    "    return tuple(el for r in M for el in r)\n",
    "excl_campaign_df[new_index_order[-1]] = excl_campaign_df[new_index_order[-1]] .apply(flatten)\n",
    "\n",
    "camps = []\n",
    "for idx,r in campaign_df.iterrows():\n",
    "    camp = {\n",
    "        \"sessions_60d\": r[\"sessions\"], \n",
    "        \"rps_avg_60d\": r[\"rps_avg\"]\n",
    "    }\n",
    "    for field,val in zip(new_index_order[:-1],idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"includes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "for idx, r in excl_campaign_df.iterrows():\n",
    "    camp = {}\n",
    "    for field, val in zip(new_index_order[:-1], idx):\n",
    "        camp[field] = {\"includes\": val}\n",
    "    last_field = new_index_order[-1]\n",
    "    camp[last_field] = {\n",
    "        \"excludes\": r[last_field]\n",
    "    }\n",
    "    camps.append(camp)\n",
    "\n",
    "camp_df = pd.DataFrame(camps)\n",
    "camp_df.to_csv(\"campaign_dump.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
